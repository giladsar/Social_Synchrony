{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9a12d6",
   "metadata": {},
   "source": [
    "### YOLO modeling\n",
    "\n",
    "after testing the fealability of using pose estimators I now move to a newer model one that can support video anylsis better then the MPI heatmap-appraoch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94156714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NumPy: 1.26.4\n",
      "âœ… OpenCV: 4.10.0\n",
      "ðŸš€ GPU Active: False\n",
      "8.3.241\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ultralytics\n",
    "\n",
    "print(f\"âœ… NumPy: {np.__version__}\")       # Expect 1.26.x\n",
    "print(f\"âœ… OpenCV: {cv2.__version__}\")     # Expect 4.10.x\n",
    "print(f\"ðŸš€ GPU Active: {torch.backends.mps.is_available()}\")\n",
    "print(ultralytics.__version__)                # Expect 8.2.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "202555c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers \n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "def show_image(img_or_list: Union[np.ndarray, List[np.ndarray]], row_plot: int = 1, titles: List[str] = None):\n",
    "    \"\"\"\n",
    "    Display one or multiple images in a grid.\n",
    "    - img_or_list: single array or list of arrays\n",
    "    - row_plot: number of rows\n",
    "    - titles: optional list of titles (same length as images)\n",
    "    \"\"\"\n",
    "    imgs = img_or_list if isinstance(img_or_list, list) else [img_or_list]\n",
    "    n = len(imgs)\n",
    "    cols = int(np.ceil(n / row_plot))\n",
    "    rows = row_plot\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    axes = np.array(axes).reshape(-1) if isinstance(axes, np.ndarray) else np.array([axes])\n",
    "\n",
    "    for i, im in enumerate(imgs):\n",
    "        if im.ndim == 2:\n",
    "            axes[i].imshow(im, cmap='gray')\n",
    "        else:\n",
    "            axes[i].imshow(im)\n",
    "        axes[i].axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            axes[i].set_title(titles[i])\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c3b08",
   "metadata": {},
   "source": [
    "### Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f47e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO # type: ignore\n",
    "\n",
    "# 1. Load the model (YOLO11 is the 2025 standard)\n",
    "model = YOLO('yolo11n-pose.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f07b0a",
   "metadata": {},
   "source": [
    "# Social Synchrony\n",
    "\n",
    "Social synchrony is operationalized as temporal coordination in movement between two interacting individuals. To quantify this coordination, we first transform pose trajectories into motion-based signals that reflect how body parts move over time - Velocity.\n",
    "\n",
    "Synchrony metrics are then computed on these motion signals rather than on raw joint positions. The primary metric examined is the *cross-correlation* between the velocity time series of the dyad, which captures the degree to which changes in movement speed are temporally aligned between partners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec2357a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Signal processing + stats\n",
    "from scipy.signal import savgol_filter, correlate\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from ultralytics import YOLO # type: ignore\n",
    "\n",
    "# -----------------------------\n",
    "# User config \n",
    "# -----------------------------\n",
    "VIDEO_PATH = \"./images/pope_and_bibi.mp4\"     # <-- change\n",
    "MODEL_WEIGHTS = \"yolov8n-pose.pt\" # or yolov8s-pose.pt for stronger results\n",
    "CONF_THRES = 0.25\n",
    "IOU_THRES  = 0.5\n",
    "\n",
    "# Rolling window for synchrony (in seconds)\n",
    "ROLL_WIN_SEC = 2.0\n",
    "\n",
    "# Max lag for cross-correlation (in seconds)\n",
    "MAX_LAG_SEC = 7.0\n",
    "\n",
    "# Joint groups (COCO-17 indices; adjust if your model differs)\n",
    "# COCO keypoints order (common): \n",
    "class BodyPart:\n",
    "    nose = 0\n",
    "    l_eye = 1\n",
    "    r_eye = 2\n",
    "    l_ear = 3\n",
    "    r_ear = 4\n",
    "    l_shoulder = 5\n",
    "    r_shoulder = 6\n",
    "    l_elbow = 7\n",
    "    r_elbow = 8\n",
    "    l_wrist = 9\n",
    "    r_wrist = 10\n",
    "    l_hip = 11\n",
    "    r_hip = 12\n",
    "    l_knee = 13\n",
    "    r_knee = 14\n",
    "    l_ankle = 15\n",
    "    r_ankle = 16\n",
    "\n",
    "body_part = BodyPart()\n",
    "UPPER_BODY = [5, 6, 7, 8, 9, 10]  # shoulders, elbows, wrists\n",
    "TORSO      = [11, 12, 5, 6]       # hips + shoulders\n",
    "LOWER_BODY = [13, 14, 15, 16]     # knees, ankles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781f7b9",
   "metadata": {},
   "source": [
    "### Loading the video \n",
    "\n",
    "since I want to asses synchrony I have you know the fps of the video processer in order for me to accurantly define what I consider to be a synchrenize movement. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "465e30fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS=29.970  dt=0.0334s\n"
     ]
    }
   ],
   "source": [
    "# --- Chunk 1: Video I/O + FPS ---\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open video.\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps is None or fps <= 1e-6:\n",
    "    # Fallback if metadata missing\n",
    "    fps = 30.0\n",
    "\n",
    "dt = 1.0 / fps\n",
    "print(f\"FPS={fps:.3f}  dt={dt:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef807ee7",
   "metadata": {},
   "source": [
    "The videoCapture function process around 30 frames per second, and alternativley every frame is 0.333 second interval from the previous one. \n",
    "\n",
    "Next, we would prcess the video through the YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a23516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1122 frames of estimators.\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(MODEL_WEIGHTS)\n",
    "\n",
    "\n",
    "output_path = \"./data_output/bibi_and_pope_skeleton.mp4\"\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # type: ignore\n",
    "\n",
    "\n",
    "# We'll store raw estimators per frame in a list.\n",
    "# Each item: a list of estimators, where estimator = (bbox_xyxy, kpts_xy, kpts_conf)\n",
    "raw_estimators: List[List[Tuple[np.ndarray, np.ndarray, np.ndarray]]] = []\n",
    "\n",
    "\n",
    "# Stuff for Video I/O\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps is None or fps <= 0:\n",
    "    fps = 30.0  # fallback (important!)\n",
    "frame_idx = 0\n",
    "    # add a video writer to fetch the skelaton video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# starting the every frame processing: \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "    # Run pose prediction\n",
    "    # Note: model(frame) returns a Results list (usually len 1 for one image)\n",
    "\n",
    "    # before passing the frame into the model I want to have a filter to improve the quality:\n",
    "    blurred_frame = cv2.GaussianBlur(frame, (7, 7), 1.0)\n",
    "    high_freq = cv2.subtract(frame.astype(float), blurred_frame.astype(float))\n",
    "    alpha = 1.5\n",
    "    sharpened = frame.astype(float) + alpha * high_freq\n",
    "    sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "        \n",
    "    results = model.predict(\n",
    "        source=sharpened,\n",
    "        conf=CONF_THRES,\n",
    "        iou=IOU_THRES,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    dets_this_frame = []\n",
    "    r0 = results[0]\n",
    "\n",
    "    # r0.boxes.xyxy -> Nx4\n",
    "    # r0.keypoints.xy -> NxKx2\n",
    "    # r0.keypoints.conf -> NxK\n",
    "    if r0.keypoints is not None and len(r0.keypoints) > 0:\n",
    "        boxes_xyxy = r0.boxes.xyxy.cpu().numpy() if r0.boxes is not None else None # type: ignore\n",
    "\n",
    "        kpts_xy    = r0.keypoints.xy.cpu().numpy() # type: ignore\n",
    "\n",
    "        kpts_conf  = r0.keypoints.conf.cpu().numpy()  # type: ignore\n",
    "\n",
    "        n_people = kpts_xy.shape[0]\n",
    "        for i in range(n_people):\n",
    "            bbox = boxes_xyxy[i] if boxes_xyxy is not None else None\n",
    "            dets_this_frame.append((bbox, kpts_xy[i], kpts_conf[i]))\n",
    "\n",
    "    raw_estimators.append(dets_this_frame)\n",
    "\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    if annotated_frame.shape[:2] != (frame_height, frame_width):\n",
    "        annotated_frame = cv2.resize(annotated_frame, (frame_width, frame_height))\n",
    "    if annotated_frame.dtype != np.uint8:\n",
    "        annotated_frame = annotated_frame.astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    # write a video frame with skeleton\n",
    "\n",
    "    out.write(annotated_frame)\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "n_frames = len(raw_estimators)\n",
    "times = np.arange(n_frames) * dt\n",
    "print(f\"Loaded {n_frames} frames of estimators.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e9bd7",
   "metadata": {},
   "source": [
    "The next section is needed only if the video contains 2 people. In my case I know that the SRL labs have 3 videos for each interactione ( 1 video containing the both side of the dyad while the other 2 videos are capturing each individual seperately). therefore, the next chulk might come handy if i would use only the dyad video - although  I do intend to use all 3 videos.\n",
    "\n",
    "\n",
    "the purpose of the next chulk enables the model to overcome of seeing the world in snapshots-  with it, the model understands a sequence. that means that since the video is divided into frames we need to form a way in which the model now that the person in frame(n ) and the person in frame(n+1) is the same person. basically a Name Tag phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296264f",
   "metadata": {},
   "source": [
    "### Identity tracking for dyads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74314a9",
   "metadata": {},
   "source": [
    "First, let Understand whats this model yields: \n",
    "the first parameter is the frame index (remember that the model run 30 fps )and the second parameter is the Person. within each frame (element in the list) and person index theres 17 elements findicating the joint of the corresponding body part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b1249da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     835.67,      146.38,      1205.7,      711.48], dtype=float32),\n",
       " array([[     941.85,      236.04],\n",
       "        [      960.4,      220.52],\n",
       "        [     942.35,      221.27],\n",
       "        [     1015.2,      217.07],\n",
       "        [     953.39,      217.28],\n",
       "        [     1108.5,      288.39],\n",
       "        [     924.54,      282.73],\n",
       "        [     1170.4,      423.26],\n",
       "        [     893.06,      392.18],\n",
       "        [     1042.2,      467.35],\n",
       "        [     948.72,      449.32],\n",
       "        [     1088.8,      512.63],\n",
       "        [     967.74,         500],\n",
       "        [     1045.8,       552.1],\n",
       "        [     889.67,      534.57],\n",
       "        [     1066.2,      719.88],\n",
       "        [     960.31,      702.57]], dtype=float32),\n",
       " array([    0.96756,     0.96015,     0.73042,     0.93046,     0.15025,     0.99595,     0.97182,     0.98812,     0.88464,     0.98271,     0.88518,     0.98961,     0.97616,     0.93903,     0.87199,     0.65975,     0.54928], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_estimators[1001][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d02bd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for tracking\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "\n",
    "#bbox_centroid-  compute centroid from bbox - each box represents [x1,y1,x2,y2] which are the top-left and bottom-right corners\n",
    "# each box is a person detected in the frame\n",
    "def bbox_centroid(bbox_xyxy: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute centroid from [x1,y1,x2,y2].\"\"\"\n",
    "    x1, y1, x2, y2 = bbox_xyxy\n",
    "    return np.array([(x1 + x2) / 2.0, (y1 + y2) / 2.0], dtype=float)\n",
    "#----------------------------\n",
    "\n",
    "#bbox_area- compute area from bbox\n",
    "def bbox_area(bbox_xyxy: np.ndarray) -> float:\n",
    "    \"\"\"Compute area of bounding box.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox_xyxy\n",
    "    return max(0.0, x2 - x1) * max(0.0, y2 - y1)\n",
    "#----------------------------\n",
    "# Greedy assignment function - assigns detected people in current frame to those in previous frame based on centroid distances\n",
    "def greedy_assign(prev_centroids: List[np.ndarray],\n",
    "                  curr_centroids: List[np.ndarray]) -> List[Optional[int]]:\n",
    "    \"\"\"\n",
    "    Greedy assignment: returns mapping from prev index -> curr index (or None).\n",
    "    \"\"\"\n",
    "    if len(prev_centroids) == 0 or len(curr_centroids) == 0:\n",
    "        return [None] * len(prev_centroids)\n",
    "\n",
    "    # Distance matrix (prev x curr)\n",
    "    D = np.zeros((len(prev_centroids), len(curr_centroids)), dtype=float)\n",
    "    for i, pc in enumerate(prev_centroids):\n",
    "        for j, cc in enumerate(curr_centroids):\n",
    "            D[i, j] = np.linalg.norm(pc - cc)\n",
    "\n",
    "    assigned_curr = set()\n",
    "    mapping = [None] * len(prev_centroids)\n",
    "\n",
    "    for _ in range(min(len(prev_centroids), len(curr_centroids))):\n",
    "        # Find the global minimum distance remaining in D\n",
    "        i, j = np.unravel_index(np.argmin(D), D.shape)\n",
    "        if D[i, j] == np.inf:\n",
    "            break\n",
    "            \n",
    "        mapping[i] = j \n",
    "        assigned_curr.add(j)\n",
    "        # \"Remove\" this row and column from future consideration\n",
    "        D[i, :] = np.inf\n",
    "        D[:, j] = np.inf\n",
    "\n",
    "    return mapping # pyright: ignore[reportReturnType]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3072fd2",
   "metadata": {},
   "source": [
    "First let's create an empty list catch the raw_estimators for 2 persons given that the input would include a dyad so no need to over-complecate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5314f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Tracking two main subjects (IDs 0 and 1) ---\n",
    "# 1. Initialize data containers\n",
    "K = 17  # Standard COCO keypoints\n",
    "n_frames = len(raw_estimators)\n",
    "tracks = {\n",
    "    0: {\"kpts_xy\": np.full((n_frames, K, 2), np.nan, dtype=float),\n",
    "        \"kpts_conf\": np.full((n_frames, K), np.nan, dtype=float)},\n",
    "    1: {\"kpts_xy\": np.full((n_frames, K, 2), np.nan, dtype=float),\n",
    "        \"kpts_conf\": np.full((n_frames, K), np.nan, dtype=float)},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50ffe6",
   "metadata": {},
   "source": [
    "then  I would find the FIRST frame that actually contains a detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4caadd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tracking from frame index: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "first_valid_idx = None\n",
    "for i, frame_dets in enumerate(raw_estimators):\n",
    "    if len(frame_dets) > 0:\n",
    "        first_valid_idx = i\n",
    "        break\n",
    "\n",
    "if first_valid_idx is None:\n",
    "    raise RuntimeError(\"No detections found in any frame. Check your video or CONF_THRES.\")\n",
    "\n",
    "print(f\"Starting tracking from frame index: {first_valid_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6b1f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Initialize IDs 0 and 1 from that first valid frame (by largest area)\n",
    "\n",
    "first_frame_dets = raw_estimators[first_valid_idx]\n",
    "\n",
    "\n",
    "# Sort by area so the main subjects get IDs 0 and 1 - the largest two bboxes \n",
    "first_sorted = sorted(first_frame_dets, key=lambda d: bbox_area(d[0]) if d[0] is not None else 0, reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "# Assign to tracks - getting the condinate of the first frame identified of the 2 peson and STORE by SIZE\n",
    "# arranging the first_sorted(which is raw_estimetors) so that the \"0\" would be the one how is the larger box in the \n",
    "#first frame identified with people\n",
    "\n",
    "# so here we have the keypoints of the first frame identified of the 2 peson and STORE by SIZE\n",
    "for pid in [0, 1]:\n",
    "    if pid < len(first_sorted):\n",
    "        bbox, kxy, kcf = first_sorted[pid]\n",
    "        tracks[pid][\"kpts_xy\"][first_valid_idx] = kxy\n",
    "        tracks[pid][\"kpts_conf\"][first_valid_idx] = kcf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac9fb6",
   "metadata": {},
   "source": [
    "Now with the heavy lifting; It plays the video forward and ensures that \"Person 0\" in Frame 10 remains \"Person 0\" in Frame 11, even if they move.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a0d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking done (dyad IDs: 0 and 1).\n",
      "Mapping contents:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'kpts_xy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m#i want to see what inside mapping, write a code that gives the stored that inside mapping neatly \u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMapping contents:\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mmapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkpts_xy\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'kpts_xy'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Main Tracking Loop\n",
    "\n",
    "for t in range(first_valid_idx + 1, n_frames): # It starts exactly one frame after your \"Casting\" frame and runs until the end of the video.\n",
    "    dets = raw_estimators[t]\n",
    "    if not dets:\n",
    "        continue\n",
    "\n",
    "    # Prepare previous centroids for matching\n",
    "    prev_centroids = []\n",
    "    prev_valid_pids = []\n",
    "\n",
    "    # The \"Memory\" Phase (Where were they?):\n",
    "    for pid in [0, 1]:\n",
    "        # Get last known position from the tracks we are building\n",
    "        last_kpts = tracks[pid][\"kpts_xy\"][t-1]\n",
    "        if not np.all(np.isnan(last_kpts)):\n",
    "            # Use the bounding box of the keypoints as the centroid\n",
    "            x1, y1 = np.nanmin(last_kpts[:, 0]), np.nanmin(last_kpts[:, 1])\n",
    "            x2, y2 = np.nanmax(last_kpts[:, 0]), np.nanmax(last_kpts[:, 1])\n",
    "            prev_centroids.append(np.array([(x1+x2)/2, (y1+y2)/2]))\n",
    "            prev_valid_pids.append(pid)\n",
    "\n",
    "    # Prepare current detections\n",
    "    curr_bboxes = [d[0] for d in dets if d[0] is not None]\n",
    "    curr_centroids = [bbox_centroid(b) for b in curr_bboxes]\n",
    "\n",
    "    if not curr_centroids or not prev_centroids:\n",
    "        # Fallback: if matching fails, just assign first available detections\n",
    "        for pid in [0, 1]:\n",
    "            if pid < len(dets):\n",
    "                _, kxy, kcf = dets[pid]\n",
    "                tracks[pid][\"kpts_xy\"][t] = kxy\n",
    "                tracks[pid][\"kpts_conf\"][t] = kcf\n",
    "        continue\n",
    "\n",
    "    # Match previous frame people to current frame people\n",
    "    mapping = greedy_assign(prev_centroids, curr_centroids)\n",
    "\n",
    "    used_det_indices = set()\n",
    "    for i, j in enumerate(mapping):\n",
    "        if j is None: continue\n",
    "        \n",
    "        pid = prev_valid_pids[i]\n",
    "        target_bbox = curr_bboxes[j]\n",
    "        \n",
    "        # Find which detection index in 'dets' this belongs to\n",
    "        for di, d in enumerate(dets):\n",
    "            if d[0] is not None and np.allclose(d[0], target_bbox) and di not in used_det_indices:\n",
    "                used_det_indices.add(di)\n",
    "                _, kxy, kcf = d\n",
    "                tracks[pid][\"kpts_xy\"][t] = kxy\n",
    "                tracks[pid][\"kpts_conf\"][t] = kcf\n",
    "                break\n",
    "\n",
    "print(\"Tracking done (dyad IDs: 0 and 1).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283232d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2461eac5",
   "metadata": {},
   "source": [
    "now we have  tracks object is a dictionary type containing two main matrices for each person - keypooint and confidance.\n",
    "If my video is 30 seconds long at 30 FPS, you have 900 rows of data. every row is a frmae\n",
    "\n",
    "kpts_xy:\n",
    "    Rows: Frame Number (Time).\n",
    "    Columns: Body Part (Nose, Left Eye, Right Shoulder, etc.).\n",
    "    The values are the coordinates $(x, y)$.\n",
    "kpts_conf:\n",
    "    Rows: Frame Number (Time).\n",
    "    Columns: Body Part (Nose, Left Eye, Right Shoulder, etc.).\n",
    "    the value is a single number (0.0 to 1.0) representing how \"confident\" the model is\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12ba6716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Structure for Person 1 ---\n",
      "XY Shape:   (1122, 17, 2)  -> (Frames, Joints, 2)\n",
      "Conf Shape: (1122, 17) -> (Frames, Joints)\n",
      "left Elbow at last frame: [     812.56      425.98]\n"
     ]
    }
   ],
   "source": [
    "pid = 1  # Check Person 0\n",
    "print(f\"--- Structure for Person {pid} ---\")\n",
    "print(f\"XY Shape:   {tracks[pid]['kpts_xy'].shape}  -> (Frames, Joints, 2)\")\n",
    "print(f\"Conf Shape: {tracks[pid]['kpts_conf'].shape} -> (Frames, Joints)\")\n",
    "\n",
    "# Example: Get the Nose (index 0) position at the last frame\n",
    "last_frame = tracks[pid]['kpts_xy'].shape[0] - 5\n",
    "l_elbow_pos_last_frame = tracks[pid]['kpts_xy'][last_frame, body_part.l_elbow] # what is index 4 \n",
    "\n",
    "print(f\"left Elbow at last frame: {l_elbow_pos_last_frame}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798701ee",
   "metadata": {},
   "source": [
    "## Preprocessing- data handling and noise reduction \n",
    "Now lets deal with noise and uncertainty \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8162a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for cleaning keypoints\n",
    "#apply_conf_mask - takes the minimun threshold given as an arguemnt and assign NaN to kpts_xy where the codition isn't met\n",
    "def apply_conf_mask(kpts_xy: np.ndarray, kpts_conf: np.ndarray, conf_min: float = 0.3) -> np.ndarray:\n",
    "    \"\"\"Set (x,y) to NaN where confidence < conf_min.\"\"\"\n",
    "    out = kpts_xy.copy()\n",
    "    bad = (kpts_conf < conf_min) | np.isnan(kpts_conf)\n",
    "    out[bad, :] = np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def interp_nan_1d(y: np.ndarray, max_gap: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Interpolate NaNs in a 1D array; only fill gaps <= max_gap.\n",
    "    \"\"\"\n",
    "    y2 = y.copy()# Work on a copy so we don't change the original data by accident\n",
    "    n = len(y2) # How long is the data?\n",
    "    idx = np.arange(n)# Create an index: [0, 1, 2, 3, ... n-1]\n",
    "\n",
    "    nan_mask = np.isnan(y2) #This creates a \"mask\" (True/False list):\n",
    "    #True: The value is missing (NaN). \n",
    "    # False: The value is real (e.g., 0.95).\n",
    "\n",
    "    if nan_mask.all():\n",
    "        return y2 # this check if the y2 pbject is ALL NaN \n",
    "\n",
    "    \n",
    "    # Interpolate all NaNs first\n",
    "    y2[nan_mask] = np.interp(idx[nan_mask], idx[~nan_mask], y2[~nan_mask]) # take the indexes of the NaN values and replace them with interpolated values from the non-NaN values (the)\n",
    "    # 1st arg (x): The \"Questions\" -> The Frame numbers (indices) where data is MISSING. - idx[nan_mask]\n",
    "    # 2nd arg (xp): The \"X-Ref\" -> The Frame numbers where we HAVE valid data. - idx[~nan_mask]\n",
    "    # 3rd arg (fp): The \"Y-Ref\" -> The actual values at those valid frames. - y2[~nan_mask]\n",
    "\n",
    "    \n",
    "    # Find consecutive NaN runs in original\n",
    "\n",
    "    runs = []\n",
    "    start = None\n",
    "    for i in range(n):\n",
    "        if nan_mask[i] and start is None:\n",
    "            start = i\n",
    "        if (not nan_mask[i]) and start is not None:\n",
    "            runs.append((start, i-1))\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        runs.append((start, n-1))\n",
    "\n",
    "    for a, b in runs:\n",
    "        if (b - a + 1) > max_gap:\n",
    "            y2[a:b+1] = np.nan\n",
    "\n",
    "    return y2\n",
    "\n",
    "def smooth_1d(y: np.ndarray, window: int = 11, poly: int = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Savitzky-Golay smoothing, safe with NaNs (smooth only the valid part).\n",
    "    \"\"\"\n",
    "    y2 = y.copy()\n",
    "    valid = ~np.isnan(y2)\n",
    "    if valid.sum() < window:\n",
    "        return y2\n",
    "    y2[valid] = savgol_filter(y2[valid], window_length=window, polyorder=poly)\n",
    "    return y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb505214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoints cleaned (confidence mask + interpolation + smoothing).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clean each person, each joint, both x and y\n",
    "CONF_THER= 0.3\n",
    "MAX_GAP_FRAMES = int(0.3 * fps)  # fill up to ~300ms gaps\n",
    "\n",
    "clean_xy = {}\n",
    "for pid in [0, 1]:\n",
    "    xy = tracks[pid][\"kpts_xy\"]          # (T,K,2)\n",
    "    cf = tracks[pid][\"kpts_conf\"]        # (T,K)\n",
    "\n",
    "    # Apply confidence mask frame-by-frame\n",
    "    xy_masked = np.full_like(xy, np.nan)\n",
    "    for t in range(n_frames):\n",
    "        xy_masked[t] = apply_conf_mask(xy[t], cf[t], conf_min=CONF_THER)\n",
    "\n",
    "    # Interpolate + smooth per joint, per coordinate over time\n",
    "    xy_filt = xy_masked.copy()\n",
    "    for j in range(K):\n",
    "        for c in [0, 1]:  # x,y\n",
    "            series = xy_masked[:, j, c]\n",
    "            series = interp_nan_1d(series, max_gap=MAX_GAP_FRAMES)\n",
    "            series = smooth_1d(series, window=11, poly=2)\n",
    "            xy_filt[:, j, c] = series\n",
    "\n",
    "    clean_xy[pid] = xy_filt\n",
    "\n",
    "print(\"Keypoints cleaned (confidence mask + interpolation + smoothing).\")\n",
    "\n",
    "# what is clean_xy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390b3b6",
   "metadata": {},
   "source": [
    "`clean_xy` is the cleaned pose time-series that we carry forward for all synchrony analyses. It contains the estimated (x, y) coordinates of each joint for each tracked person at every frame, after low-confidence detections were removed, short gaps were interpolated, and frame-to-frame jitter was smoothed.\n",
    "\n",
    " From this point on, we do **not** work with raw YOLO outputs anymoreâ€”`clean_xy` is treated as our best estimate of true body motion. \n",
    " \n",
    " Because social synchrony is defined in terms of **coordinated movement over time** rather than static posture or absolute position, the next step is to transform `clean_xy` into motion signals (e.g., joint velocity magnitudes) that capture how each body part moves from frame to frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0539b",
   "metadata": {},
   "source": [
    "The function `velocity_magnitude()` transforms joint positions over time into a one-dimensional motion signal by computing frame-to-frame velocity. \n",
    "\n",
    "For each time point, it estimates horizontal and vertical velocity components using finite differences and combines them into a single scalar velocity magnitude. This representation captures how fast a joint is moving regardless of direction, making it well-suited for comparing movement dynamics between individuals and for computing synchrony metrics. Missing values are propagated to avoid introducing spurious motion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/swb4gsnx7p564yccbz0s91d80000gp/T/ipykernel_22458/3614257110.py:28: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(all_v, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion signals extracted: upper/torso/lower velocity magnitudes.\n"
     ]
    }
   ],
   "source": [
    "# --- Chunk 5: Pose -> motion signals (velocity magnitude) ---\n",
    "\n",
    "def velocity_magnitude(xy: np.ndarray, dt: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    xy: (T,2) joint coordinates over time\n",
    "    returns v: (T,) velocity magnitude, NaN where insufficient data\n",
    "    \"\"\"\n",
    "    v = np.full((xy.shape[0],), np.nan, dtype=float)\n",
    "    # Finite difference: v[t] based on xy[t] - xy[t-1]\n",
    "    for t in range(1, xy.shape[0]):\n",
    "        if np.any(np.isnan(xy[t])) or np.any(np.isnan(xy[t-1])):\n",
    "            continue\n",
    "        dx = (xy[t, 0] - xy[t-1, 0]) / dt\n",
    "        dy = (xy[t, 1] - xy[t-1, 1]) / dt\n",
    "        v[t] = np.sqrt(dx*dx + dy*dy)\n",
    "    return v\n",
    "\n",
    "def group_velocity(clean_xy_pid: np.ndarray, joint_ids: List[int], dt: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute per-joint velocity magnitude and average across the joint group.\n",
    "    Output: (T,) group velocity signal.\n",
    "    \"\"\"\n",
    "    all_v = []\n",
    "    for j in joint_ids:\n",
    "        vj = velocity_magnitude(clean_xy_pid[:, j, :], dt)\n",
    "        all_v.append(vj)\n",
    "    all_v = np.vstack(all_v)  # (len(joints), T)\n",
    "    return np.nanmean(all_v, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c43756",
   "metadata": {},
   "source": [
    "Although pose estimation provides joint-level data, synchrony was not analyzed independently for every individual body part. Joint-level signals are inherently noisy, frequently affected by occlusion, and yield highly correlated time series across neighboring joints (e.g., wristâ€“elbowâ€“shoulder). Analyzing each joint separately would therefore substantially increase the number of statistical tests without providing independent information, inflating false-positive risk and reducing interpretability.\n",
    "\n",
    "\n",
    "Instead, joints were grouped into anatomically and functionally meaningful regions (upper body, torso, lower body), which improves signal-to-noise ratio, preserves statistical power, and yields synchrony measures that are more stable and theoretically interpretable at the interaction level.\n",
    " \n",
    "Joint-wise analyses can be conducted as exploratory or supplementary analyses, but region-level aggregation is more appropriate for primary inference.\n",
    "\n",
    "Therefore, the mean of every body part's velocity within each group would the the metric  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e5e9841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/swb4gsnx7p564yccbz0s91d80000gp/T/ipykernel_22458/3614257110.py:28: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(all_v, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion signals extracted: upper/torso/lower velocity magnitudes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Body_Parts_Group_Velocity = {}\n",
    "for pid in [0, 1]:\n",
    "    Body_Parts_Group_Velocity[pid] = {\n",
    "        \"upper_v\": group_velocity(clean_xy[pid], UPPER_BODY, dt),\n",
    "        \"torso_v\": group_velocity(clean_xy[pid], TORSO, dt),\n",
    "        \"lower_v\": group_velocity(clean_xy[pid], LOWER_BODY, dt),\n",
    "    }\n",
    "\n",
    "print(\"Motion signals extracted: upper/torso/lower velocity magnitudes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b6e0a",
   "metadata": {},
   "source": [
    "After establishing  a quantifiable measure for movement it's time to constract out first metric: cross-correlation with lag (correlation between Person A and Person B motion signals while shifting time (lag)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "697b580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for cross-correlation analysis\n",
    "def zscore_nan(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Z-score while ignoring NaNs.\"\"\"\n",
    "    x2 = x.copy()\n",
    "    m = np.nanmean(x2)\n",
    "    s = np.nanstd(x2)\n",
    "    if np.isnan(m) or np.isnan(s) or s < 1e-8:\n",
    "        return x2 * np.nan\n",
    "    return (x2 - m) / s\n",
    "\n",
    "def lagged_corr(x: np.ndarray, y: np.ndarray, max_lag_frames: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute correlation r(lag) for lags in [-max_lag_frames, +max_lag_frames].\n",
    "    Returns lags (frames) and correlations (r).\n",
    "    \"\"\"\n",
    "    lags = np.arange(-max_lag_frames, max_lag_frames + 1)\n",
    "    rs = np.full_like(lags, np.nan, dtype=float)\n",
    "\n",
    "    for i, lag in enumerate(lags):\n",
    "        if lag < 0:\n",
    "            x_seg = x[-lag:]\n",
    "            y_seg = y[:len(y)+lag]\n",
    "        elif lag > 0:\n",
    "            x_seg = x[:len(x)-lag]\n",
    "            y_seg = y[lag:]\n",
    "        else:\n",
    "            x_seg = x\n",
    "            y_seg = y\n",
    "\n",
    "        # Mask NaNs pairwise\n",
    "        mask = ~np.isnan(x_seg) & ~np.isnan(y_seg)\n",
    "        if mask.sum() < 10:\n",
    "            continue\n",
    "\n",
    "        r, _ = pearsonr(x_seg[mask], y_seg[mask])\n",
    "        rs[i] = r\n",
    "\n",
    "    return lags, rs\n",
    "\n",
    "def summarize_lagged_sync(x: np.ndarray, y: np.ndarray, fps: float, max_lag_sec: float) -> Dict[str, float]:\n",
    "    max_lag_frames = int(max_lag_sec * fps)\n",
    "    xz = zscore_nan(x)\n",
    "    yz = zscore_nan(y)\n",
    "\n",
    "    lags, rs = lagged_corr(xz, yz, max_lag_frames=max_lag_frames)\n",
    "\n",
    "    if np.all(np.isnan(rs)):\n",
    "        return {\"max_r\": np.nan, \"lag_sec\": np.nan}\n",
    "\n",
    "    best_i = np.nanargmax(rs)\n",
    "    best_lag_frames = lags[best_i]\n",
    "    return {\"max_r\": float(rs[best_i]), \"lag_sec\": float(best_lag_frames / fps)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9954fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagged synchrony (max within Â±lag):\n",
      "upper: {'max_r': 0.5427069867133023, 'lag_sec': 0.0}\n",
      "torso: {'max_r': 0.30266748008142197, 'lag_sec': -0.10010000000000001}\n",
      "lower: {'max_r': 0.41398075580228166, 'lag_sec': 0.46713333333333334}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: upper-body synchrony\n",
    "sync_upper = summarize_lagged_sync(signals[0][\"upper_v\"], signals[1][\"upper_v\"], fps=fps, max_lag_sec=MAX_LAG_SEC)\n",
    "sync_torso = summarize_lagged_sync(signals[0][\"torso_v\"], signals[1][\"torso_v\"], fps=fps, max_lag_sec=MAX_LAG_SEC)\n",
    "sync_lower = summarize_lagged_sync(signals[0][\"lower_v\"], signals[1][\"lower_v\"], fps=fps, max_lag_sec=MAX_LAG_SEC)\n",
    "\n",
    "print(\"Lagged synchrony (max within Â±lag):\")\n",
    "print(\"upper:\", sync_upper)\n",
    "print(\"torso:\", sync_torso)\n",
    "print(\"lower:\", sync_lower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62ea70",
   "metadata": {},
   "source": [
    "Let's see if such correlation is significant using an a parametric test - Permutation.\n",
    "I didn't chose t test since such test assumes independence whereas in an interaction one person movement effect and influance the other. \n",
    "\n",
    "Core idea of the permutation test:\n",
    "\n",
    "* Keep each personâ€™s movement intact\n",
    "\n",
    "* Destroy interpersonal timing by shifting one signal in time\n",
    "\n",
    "* Recompute the same statistic (max cross-correlation) many times\n",
    "\n",
    "* Compare the real value to this null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def circular_shift(x: np.ndarray, shift: int) -> np.ndarray:\n",
    "    \"\"\"Circularly shift a 1D array by 'shift' frames.\"\"\"\n",
    "    return np.roll(x, shift)\n",
    "\n",
    "def synchrony_null_distribution(x: np.ndarray, y: np.ndarray, fps: float, max_lag_sec: float,\n",
    "                                n_perm: int = 500, min_shift_sec: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build null distribution of max cross-corr by circularly shifting y.\n",
    "    min_shift_sec prevents tiny shifts that preserve coupling.\n",
    "    \"\"\"\n",
    "    max_lag_frames = int(max_lag_sec * fps)\n",
    "    min_shift = int(min_shift_sec * fps)\n",
    "\n",
    "    xz = zscore_nan(x)\n",
    "    yz = zscore_nan(y)\n",
    "\n",
    "    null_vals = np.full((n_perm,), np.nan, dtype=float) # building an empty array to fetch latter on the lagged_cor\n",
    "    for i in range(n_perm):\n",
    "        # sample a shift far enough away\n",
    "        shift = np.random.randint(min_shift, len(yz) - min_shift)\n",
    "        y_shift = circular_shift(yz, shift)\n",
    "\n",
    "        _, rs = lagged_corr(xz, y_shift, max_lag_frames)\n",
    "        null_vals[i] = np.nanmax(rs)\n",
    "\n",
    "    return null_vals\n",
    "\n",
    "def p_value_from_null(real_val: float, null_vals: np.ndarray) -> float:\n",
    "    \"\"\"One-sided p: probability null >= real.\"\"\"\n",
    "    null_vals = null_vals[~np.isnan(null_vals)]\n",
    "    if len(null_vals) == 0 or np.isnan(real_val):\n",
    "        return np.nan\n",
    "    return float((np.sum(null_vals >= real_val) + 1) / (len(null_vals) + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d45a5",
   "metadata": {},
   "source": [
    "Let's check if H1 hypothesis is being supported!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13381d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apa_row(region_name, sync_dict, signal_a, signal_b):\n",
    "    real_r = sync_dict[\"max_r\"]\n",
    "\n",
    "    null_vals = synchrony_null_distribution(\n",
    "        signal_a,\n",
    "        signal_b,\n",
    "        fps=fps,\n",
    "        max_lag_sec=MAX_LAG_SEC,\n",
    "        n_perm=300,\n",
    "        min_shift_sec=1.0\n",
    "    )\n",
    "\n",
    "    p_val = p_value_from_null(real_r, null_vals)\n",
    "\n",
    "    return {\n",
    "        \"Body region\": region_name,\n",
    "        \"Observed r\": round(real_r, 2),\n",
    "        \"Null mean r\": round(float(np.nanmean(null_vals)), 2),\n",
    "        \"p (perm)\": round(p_val, 3)\n",
    "    }\n",
    "\n",
    "apa_table = pd.DataFrame([\n",
    "    apa_row(\"Upper body\", sync_upper, signals[0][\"upper_v\"], signals[1][\"upper_v\"]),\n",
    "    apa_row(\"Torso\",      sync_torso, signals[0][\"torso_v\"], signals[1][\"torso_v\"]),\n",
    "    apa_row(\"Lower body\", sync_lower, signals[0][\"lower_v\"], signals[1][\"lower_v\"]),\n",
    "])\n",
    "\n",
    "apa_table\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
