{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9a12d6",
   "metadata": {},
   "source": [
    "### YOLO modeling\n",
    "\n",
    "after testing the fealability of using pose estimators I now move to a newer model one that can support video anylsis better then the MPI heatmap-appraoch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94156714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NumPy: 1.26.4\n",
      "âœ… OpenCV: 4.10.0\n",
      "ðŸš€ GPU Active: False\n",
      "8.3.241\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ultralytics\n",
    "\n",
    "print(f\"âœ… NumPy: {np.__version__}\")       # Expect 1.26.x\n",
    "print(f\"âœ… OpenCV: {cv2.__version__}\")     # Expect 4.10.x\n",
    "print(f\"ðŸš€ GPU Active: {torch.backends.mps.is_available()}\")\n",
    "print(ultralytics.__version__)                # Expect 8.2.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202555c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers \n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "def show_image(img_or_list: Union[np.ndarray, List[np.ndarray]], row_plot: int = 1, titles: List[str] = None):\n",
    "    \"\"\"\n",
    "    Display one or multiple images in a grid.\n",
    "    - img_or_list: single array or list of arrays\n",
    "    - row_plot: number of rows\n",
    "    - titles: optional list of titles (same length as images)\n",
    "    \"\"\"\n",
    "    imgs = img_or_list if isinstance(img_or_list, list) else [img_or_list]\n",
    "    n = len(imgs)\n",
    "    cols = int(np.ceil(n / row_plot))\n",
    "    rows = row_plot\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    axes = np.array(axes).reshape(-1) if isinstance(axes, np.ndarray) else np.array([axes])\n",
    "\n",
    "    for i, im in enumerate(imgs):\n",
    "        if im.ndim == 2:\n",
    "            axes[i].imshow(im, cmap='gray')\n",
    "        else:\n",
    "            axes[i].imshow(im)\n",
    "        axes[i].axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            axes[i].set_title(titles[i])\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c3b08",
   "metadata": {},
   "source": [
    "### Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f47e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt to 'yolo11n-pose.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.0MB 7.9MB/s 0.8s0.7s<0.1s.2ss\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO # type: ignore\n",
    "\n",
    "# 1. Load the model (YOLO11 is the 2025 standard)\n",
    "model = YOLO('yolo11n-pose.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd81dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: ./images/dancing.mp4\n",
      "WARNING âš ï¸ not enough matching points\n",
      "WARNING âš ï¸ not enough matching points\n",
      "WARNING âš ï¸ not enough matching points\n",
      "Done! The video with skeletons has been saved to: ./data_output/output_skeleton.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "# . Open the input video\n",
    "video_path = \"./images/dancing.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# --- VIDEO WRITER SETUP ---\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "output_path = \"./data_output/output_skeleton.mp4\"\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "print(f\"Processing video: {video_path}\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 3. Run Pose Tracking\n",
    "    # device='mps' uses the Mac GPU for faster processing\n",
    "    results = model.track(\n",
    "        frame,\n",
    "        device='mps',\n",
    "        persist=True,\n",
    "        verbose=False,\n",
    "        tracker='bytetrack.yaml' \n",
    "    )\n",
    "    \n",
    "    # 4. Draw the Skeleton\n",
    "    # .plot() draws bounding boxes and the skeleton (keypoints + lines)\n",
    "    annotated_frame = results[0].plot() \n",
    "    \n",
    "    # 5. Write the frame to the output video file\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "# 6. Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Done! The video with skeletons has been saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f07b0a",
   "metadata": {},
   "source": [
    "# Social Synchrony\n",
    "The first metric I would examine would be joint velocity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2357a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Signal processing + stats\n",
    "from scipy.signal import savgol_filter, correlate\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from ultralytics import YOLO # type: ignore\n",
    "\n",
    "# -----------------------------\n",
    "# User config \n",
    "# -----------------------------\n",
    "VIDEO_PATH = \"./images/dancing.mp4\"     # <-- change\n",
    "MODEL_WEIGHTS = \"yolov8n-pose.pt\" # or yolov8s-pose.pt for stronger results\n",
    "CONF_THRES = 0.25\n",
    "IOU_THRES  = 0.5\n",
    "\n",
    "# Rolling window for synchrony (in seconds)\n",
    "ROLL_WIN_SEC = 2.0\n",
    "\n",
    "# Max lag for cross-correlation (in seconds)\n",
    "MAX_LAG_SEC = 1.0\n",
    "\n",
    "# Joint groups (COCO-17 indices; adjust if your model differs)\n",
    "# COCO keypoints order (common): \n",
    "# 0 nose, \n",
    "# 1 l_eye, \n",
    "# 2 r_eye, \n",
    "# 3 l_ear, \n",
    "# 4 r_ear,\n",
    "# 5 l_shoulder,\n",
    "#  6 r_shoulder,\n",
    "#  7 l_elbow, \n",
    "# 8 r_elbow, \n",
    "# 9  l_wrist, \n",
    "# 10 r_wrist,\n",
    "# 11 l_hip,\n",
    "#  12 r_hip, \n",
    "# 13 l_knee, \n",
    "# 14 r_knee, \n",
    "# 15 l_ankle, \n",
    "# 16 r_ankle\n",
    "UPPER_BODY = [5, 6, 7, 8, 9, 10]  # shoulders, elbows, wrists\n",
    "TORSO      = [11, 12, 5, 6]       # hips + shoulders\n",
    "LOWER_BODY = [13, 14, 15, 16]     # knees, ankles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781f7b9",
   "metadata": {},
   "source": [
    "### Loading the video \n",
    "\n",
    "since I want to asses synchrony I have you know the fps of the video processer in order for me to accurantly define what I consider to be a synchrenize movement. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "465e30fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS=29.970  dt=0.0334s\n"
     ]
    }
   ],
   "source": [
    "# --- Chunk 1: Video I/O + FPS ---\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open video.\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps is None or fps <= 1e-6:\n",
    "    # Fallback if metadata missing\n",
    "    fps = 30.0\n",
    "\n",
    "dt = 1.0 / fps\n",
    "print(f\"FPS={fps:.3f}  dt={dt:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef807ee7",
   "metadata": {},
   "source": [
    "The videoCapture function process around 30 frames per second, and alternativley every frame is 0.333 second interval from the previous one. \n",
    "\n",
    "Next, we would prcess the video through the YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a23516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 frames of estimators.\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(MODEL_WEIGHTS)\n",
    "\n",
    "# add a video writer to fetch the skelaton video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "output_path = \"./data_output/output_skeleton2.mp4\"\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# We'll store raw estimators per frame in a list.\n",
    "# Each item: a list of estimators, where estimator = (bbox_xyxy, kpts_xy, kpts_conf)\n",
    "raw_estimators: List[List[Tuple[np.ndarray, np.ndarray, np.ndarray]]] = []\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run pose prediction\n",
    "    # Note: model(frame) returns a Results list (usually len 1 for one image)\n",
    "    results = model.predict(\n",
    "        source=frame,\n",
    "        conf=CONF_THRES,\n",
    "        iou=IOU_THRES,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    dets_this_frame = []\n",
    "    r0 = results[0]\n",
    "\n",
    "    # r0.boxes.xyxy -> Nx4\n",
    "    # r0.keypoints.xy -> NxKx2\n",
    "    # r0.keypoints.conf -> NxK\n",
    "    if r0.keypoints is not None and len(r0.keypoints) > 0:\n",
    "        boxes_xyxy = r0.boxes.xyxy.cpu().numpy() if r0.boxes is not None else None\n",
    "        kpts_xy    = r0.keypoints.xy.cpu().numpy()\n",
    "        kpts_conf  = r0.keypoints.conf.cpu().numpy()\n",
    "\n",
    "        n_people = kpts_xy.shape[0]\n",
    "        for i in range(n_people):\n",
    "            bbox = boxes_xyxy[i] if boxes_xyxy is not None else None\n",
    "            dets_this_frame.append((bbox, kpts_xy[i], kpts_conf[i]))\n",
    "\n",
    "    raw_estimators.append(dets_this_frame)\n",
    "    frame_idx += 1\n",
    "    # write a video frame with skeleton\n",
    "    annotated_frame = results[0].plot()\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "n_frames = len(raw_estimators)\n",
    "times = np.arange(n_frames) * dt\n",
    "print(f\"Loaded {n_frames} frames of estimators.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
