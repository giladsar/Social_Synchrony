{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9a12d6",
   "metadata": {},
   "source": [
    "### YOLO modeling\n",
    "\n",
    "after testing the fealability of using pose estimators I now move to a newer model one that can support video anylsis better then the MPI heatmap-appraoch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94156714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NumPy: 1.26.4\n",
      "âœ… OpenCV: 4.10.0\n",
      "ðŸš€ GPU Active: False\n",
      "8.3.241\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ultralytics\n",
    "\n",
    "print(f\"âœ… NumPy: {np.__version__}\")       # Expect 1.26.x\n",
    "print(f\"âœ… OpenCV: {cv2.__version__}\")     # Expect 4.10.x\n",
    "print(f\"ðŸš€ GPU Active: {torch.backends.mps.is_available()}\")\n",
    "print(ultralytics.__version__)                # Expect 8.2.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202555c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers \n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "def show_image(img_or_list: Union[np.ndarray, List[np.ndarray]], row_plot: int = 1, titles: List[str] = None):\n",
    "    \"\"\"\n",
    "    Display one or multiple images in a grid.\n",
    "    - img_or_list: single array or list of arrays\n",
    "    - row_plot: number of rows\n",
    "    - titles: optional list of titles (same length as images)\n",
    "    \"\"\"\n",
    "    imgs = img_or_list if isinstance(img_or_list, list) else [img_or_list]\n",
    "    n = len(imgs)\n",
    "    cols = int(np.ceil(n / row_plot))\n",
    "    rows = row_plot\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    axes = np.array(axes).reshape(-1) if isinstance(axes, np.ndarray) else np.array([axes])\n",
    "\n",
    "    for i, im in enumerate(imgs):\n",
    "        if im.ndim == 2:\n",
    "            axes[i].imshow(im, cmap='gray')\n",
    "        else:\n",
    "            axes[i].imshow(im)\n",
    "        axes[i].axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            axes[i].set_title(titles[i])\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c3b08",
   "metadata": {},
   "source": [
    "### Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f47e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO # type: ignore\n",
    "\n",
    "# 1. Load the model (YOLO11 is the 2025 standard)\n",
    "model = YOLO('yolo11n-pose.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f07b0a",
   "metadata": {},
   "source": [
    "# Social Synchrony\n",
    "The first metric I would examine would be joint velocity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec2357a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Signal processing + stats\n",
    "from scipy.signal import savgol_filter, correlate\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from ultralytics import YOLO # type: ignore\n",
    "\n",
    "# -----------------------------\n",
    "# User config \n",
    "# -----------------------------\n",
    "VIDEO_PATH = \"./images/pope_and_bibi.mp4\"     # <-- change\n",
    "MODEL_WEIGHTS = \"yolov8n-pose.pt\" # or yolov8s-pose.pt for stronger results\n",
    "CONF_THRES = 0.25\n",
    "IOU_THRES  = 0.5\n",
    "\n",
    "# Rolling window for synchrony (in seconds)\n",
    "ROLL_WIN_SEC = 2.0\n",
    "\n",
    "# Max lag for cross-correlation (in seconds)\n",
    "MAX_LAG_SEC = 1.0\n",
    "\n",
    "# Joint groups (COCO-17 indices; adjust if your model differs)\n",
    "# COCO keypoints order (common): \n",
    "# 0 nose, \n",
    "# 1 l_eye, \n",
    "# 2 r_eye, \n",
    "# 3 l_ear, \n",
    "# 4 r_ear,\n",
    "# 5 l_shoulder,\n",
    "#  6 r_shoulder,\n",
    "#  7 l_elbow, \n",
    "# 8 r_elbow, \n",
    "# 9  l_wrist, \n",
    "# 10 r_wrist,\n",
    "# 11 l_hip,\n",
    "#  12 r_hip, \n",
    "# 13 l_knee, \n",
    "# 14 r_knee, \n",
    "# 15 l_ankle, \n",
    "# 16 r_ankle\n",
    "UPPER_BODY = [5, 6, 7, 8, 9, 10]  # shoulders, elbows, wrists\n",
    "TORSO      = [11, 12, 5, 6]       # hips + shoulders\n",
    "LOWER_BODY = [13, 14, 15, 16]     # knees, ankles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781f7b9",
   "metadata": {},
   "source": [
    "### Loading the video \n",
    "\n",
    "since I want to asses synchrony I have you know the fps of the video processer in order for me to accurantly define what I consider to be a synchrenize movement. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465e30fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS=29.970  dt=0.0334s\n"
     ]
    }
   ],
   "source": [
    "# --- Chunk 1: Video I/O + FPS ---\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open video.\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps is None or fps <= 1e-6:\n",
    "    # Fallback if metadata missing\n",
    "    fps = 30.0\n",
    "\n",
    "dt = 1.0 / fps\n",
    "print(f\"FPS={fps:.3f}  dt={dt:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef807ee7",
   "metadata": {},
   "source": [
    "The videoCapture function process around 30 frames per second, and alternativley every frame is 0.333 second interval from the previous one. \n",
    "\n",
    "Next, we would prcess the video through the YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a23516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1122 frames of estimators.\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(MODEL_WEIGHTS)\n",
    "\n",
    "\n",
    "output_path = \"./data_output/bibi_and_pope_skeleton.mp4\"\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # type: ignore\n",
    "\n",
    "\n",
    "# We'll store raw estimators per frame in a list.\n",
    "# Each item: a list of estimators, where estimator = (bbox_xyxy, kpts_xy, kpts_conf)\n",
    "raw_estimators: List[List[Tuple[np.ndarray, np.ndarray, np.ndarray]]] = []\n",
    "\n",
    "\n",
    "# Stuff for Video I/O\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps is None or fps <= 0:\n",
    "    fps = 30.0  # fallback (important!)\n",
    "frame_idx = 0\n",
    "    # add a video writer to fetch the skelaton video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# starting the every frame processing: \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "    # Run pose prediction\n",
    "    # Note: model(frame) returns a Results list (usually len 1 for one image)\n",
    "\n",
    "    # before passing the frame into the model I want to have a filter to improve the quality:\n",
    "    blurred_frame = cv2.GaussianBlur(frame, (7, 7), 1.0)\n",
    "    high_freq = cv2.subtract(frame.astype(float), blurred_frame.astype(float))\n",
    "    alpha = 1.5\n",
    "    sharpened = frame.astype(float) + alpha * high_freq\n",
    "    sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "        \n",
    "    results = model.predict(\n",
    "        source=sharpened,\n",
    "        conf=CONF_THRES,\n",
    "        iou=IOU_THRES,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    dets_this_frame = []\n",
    "    r0 = results[0]\n",
    "\n",
    "    # r0.boxes.xyxy -> Nx4\n",
    "    # r0.keypoints.xy -> NxKx2\n",
    "    # r0.keypoints.conf -> NxK\n",
    "    if r0.keypoints is not None and len(r0.keypoints) > 0:\n",
    "        boxes_xyxy = r0.boxes.xyxy.cpu().numpy() if r0.boxes is not None else None # type: ignore\n",
    "\n",
    "        kpts_xy    = r0.keypoints.xy.cpu().numpy() # type: ignore\n",
    "\n",
    "        kpts_conf  = r0.keypoints.conf.cpu().numpy()  # type: ignore\n",
    "\n",
    "        n_people = kpts_xy.shape[0]\n",
    "        for i in range(n_people):\n",
    "            bbox = boxes_xyxy[i] if boxes_xyxy is not None else None\n",
    "            dets_this_frame.append((bbox, kpts_xy[i], kpts_conf[i]))\n",
    "\n",
    "    raw_estimators.append(dets_this_frame)\n",
    "\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    if annotated_frame.shape[:2] != (frame_height, frame_width):\n",
    "        annotated_frame = cv2.resize(annotated_frame, (frame_width, frame_height))\n",
    "    if annotated_frame.dtype != np.uint8:\n",
    "        annotated_frame = annotated_frame.astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    # write a video frame with skeleton\n",
    "\n",
    "    out.write(annotated_frame)\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "n_frames = len(raw_estimators)\n",
    "times = np.arange(n_frames) * dt\n",
    "print(f\"Loaded {n_frames} frames of estimators.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e9bd7",
   "metadata": {},
   "source": [
    "The next section is needed only if the video contains 2 people. In my case I know that the SRL labs have 3 videos for each interactione ( 1 video containing the both side of the dyad while the other 2 videos are capturing each individual seperately). therefore, the next chulk might come handy if i would use only the dyad video - although that I intend to use all 3 videos.\n",
    "\n",
    "\n",
    "the purpose of the next chulk enables the model to overcome of seeing the world in snapshots-  with it, the model understands a sequence. that means that since the video is divided into frames we need to form a way in which the model now that the person in frame(n ) and the person in frame(n+1) is the same person. basically a Name Tag phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296264f",
   "metadata": {},
   "source": [
    "### Identity tracking for dyads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74314a9",
   "metadata": {},
   "source": [
    "First, let Understand whats this model yields: \n",
    "the first parameter is the frame index (remember that the model run 30 fps )and the second parameter is the Person. within each frame (element in the list) and person index theres 17 elements findicating the joint of the corresponding body part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b1249da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     834.34,      143.87,      1205.4,      711.51], dtype=float32),\n",
       " array([[     943.77,      237.66],\n",
       "        [     961.87,      220.35],\n",
       "        [     941.71,      222.49],\n",
       "        [     1011.3,      216.71],\n",
       "        [     954.74,      217.16],\n",
       "        [       1105,      289.04],\n",
       "        [     933.67,       278.3],\n",
       "        [     1165.3,      422.64],\n",
       "        [     900.03,      382.93],\n",
       "        [     1041.3,      465.94],\n",
       "        [     943.89,      448.62],\n",
       "        [     1094.3,      506.03],\n",
       "        [     976.57,       493.2],\n",
       "        [     1041.6,      575.51],\n",
       "        [     893.79,      557.94],\n",
       "        [     1077.8,       715.9],\n",
       "        [     958.32,      694.53]], dtype=float32),\n",
       " array([    0.94467,      0.9245,     0.64433,     0.88572,     0.15267,     0.99369,     0.95836,     0.98598,     0.88566,     0.97664,     0.87879,     0.98274,     0.96508,     0.87681,     0.77472,     0.41018,     0.31426], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_estimators[1000][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02bd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for tracking\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "\n",
    "#bbox_centroid-  compute centroid from bbox - each box represents [x1,y1,x2,y2] which are the top-left and bottom-right corners\n",
    "# each box is a person detected in the frame\n",
    "def bbox_centroid(bbox_xyxy: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute centroid from [x1,y1,x2,y2].\"\"\"\n",
    "    x1, y1, x2, y2 = bbox_xyxy\n",
    "    return np.array([(x1 + x2) / 2.0, (y1 + y2) / 2.0], dtype=float)\n",
    "#----------------------------\n",
    "\n",
    "#bbox_area- compute area from bbox\n",
    "def bbox_area(bbox_xyxy: np.ndarray) -> float:\n",
    "    \"\"\"Compute area of bounding box.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox_xyxy\n",
    "    return max(0.0, x2 - x1) * max(0.0, y2 - y1)\n",
    "#----------------------------\n",
    "# Greedy assignment function - assigns detected people in current frame to those in previous frame based on centroid distances\n",
    "def greedy_assign(prev_centroids: List[np.ndarray],\n",
    "                  curr_centroids: List[np.ndarray]) -> List[Optional[int]]:\n",
    "    \"\"\"\n",
    "    Greedy assignment: returns mapping from prev index -> curr index (or None).\n",
    "    \"\"\"\n",
    "    if len(prev_centroids) == 0 or len(curr_centroids) == 0:\n",
    "        return [None] * len(prev_centroids)\n",
    "\n",
    "    # Distance matrix (prev x curr)\n",
    "    D = np.zeros((len(prev_centroids), len(curr_centroids)), dtype=float)\n",
    "    for i, pc in enumerate(prev_centroids):\n",
    "        for j, cc in enumerate(curr_centroids):\n",
    "            D[i, j] = np.linalg.norm(pc - cc)\n",
    "\n",
    "    assigned_curr = set()\n",
    "    mapping = [None] * len(prev_centroids)\n",
    "\n",
    "    for _ in range(min(len(prev_centroids), len(curr_centroids))):\n",
    "        # Find the global minimum distance remaining in D\n",
    "        i, j = np.unravel_index(np.argmin(D), D.shape)\n",
    "        if D[i, j] == np.inf:\n",
    "            break\n",
    "            \n",
    "        mapping[i] = j \n",
    "        assigned_curr.add(j)\n",
    "        # \"Remove\" this row and column from future consideration\n",
    "        D[i, :] = np.inf\n",
    "        D[:, j] = np.inf\n",
    "\n",
    "    return mapping # pyright: ignore[reportReturnType]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3072fd2",
   "metadata": {},
   "source": [
    "First let's create an empty list catch the raw_estimators for 2 persons given that the input would include a dyad so no need to over-complecate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5314f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Tracking two main subjects (IDs 0 and 1) ---\n",
    "# 1. Initialize data containers\n",
    "K = 17  # Standard COCO keypoints\n",
    "n_frames = len(raw_estimators)\n",
    "tracks = {\n",
    "    0: {\"kpts_xy\": np.full((n_frames, K, 2), np.nan, dtype=float),\n",
    "        \"kpts_conf\": np.full((n_frames, K), np.nan, dtype=float)},\n",
    "    1: {\"kpts_xy\": np.full((n_frames, K, 2), np.nan, dtype=float),\n",
    "        \"kpts_conf\": np.full((n_frames, K), np.nan, dtype=float)},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50ffe6",
   "metadata": {},
   "source": [
    "then  I would find the FIRST frame that actually contains a detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caadd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tracking from frame index: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "first_valid_idx = None\n",
    "for i, frame_dets in enumerate(raw_estimators):\n",
    "    if len(frame_dets) > 0:\n",
    "        first_valid_idx = i\n",
    "        break\n",
    "\n",
    "if first_valid_idx is None:\n",
    "    raise RuntimeError(\"No detections found in any frame. Check your video or CONF_THRES.\")\n",
    "\n",
    "print(f\"Starting tracking from frame index: {first_valid_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1f26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([     371.38,      104.53,      797.69,      662.71], dtype=float32),\n",
       "  array([[     551.93,      240.41],\n",
       "         [     587.16,      219.35],\n",
       "         [     539.47,      211.19],\n",
       "         [     644.91,      230.39],\n",
       "         [     522.09,      209.09],\n",
       "         [     709.83,      350.54],\n",
       "         [     453.23,      334.01],\n",
       "         [     763.28,      547.99],\n",
       "         [     437.84,      536.56],\n",
       "         [     677.92,      617.19],\n",
       "         [     567.18,      624.48],\n",
       "         [     652.79,      664.99],\n",
       "         [     479.45,      655.01],\n",
       "         [     683.69,      711.48],\n",
       "         [     453.24,      678.58],\n",
       "         [     689.54,         720],\n",
       "         [     562.87,      680.47]], dtype=float32),\n",
       "  array([    0.98782,     0.98007,     0.94901,     0.91652,     0.57789,     0.99126,     0.97965,       0.935,     0.84354,     0.87143,     0.73743,     0.72691,      0.6455,    0.054678,    0.039682,   0.0072372,    0.005859], dtype=float32)),\n",
       " (array([          0,      69.663,      273.51,      710.33], dtype=float32),\n",
       "  array([[     146.47,      206.23],\n",
       "         [      141.1,       183.2],\n",
       "         [      123.1,       181.8],\n",
       "         [     106.77,      189.06],\n",
       "         [     40.461,      193.59],\n",
       "         [     99.656,      310.86],\n",
       "         [          0,      343.56],\n",
       "         [     148.26,      477.43],\n",
       "         [          0,      575.27],\n",
       "         [     224.39,         559],\n",
       "         [     80.398,      692.71],\n",
       "         [     97.645,      651.99],\n",
       "         [     2.4707,      678.94],\n",
       "         [     230.88,      699.62],\n",
       "         [     95.146,      712.75],\n",
       "         [     211.59,         720],\n",
       "         [     53.204,         720]], dtype=float32),\n",
       "  array([    0.87883,     0.56261,     0.72986,    0.064687,     0.49027,     0.96686,     0.40963,     0.94585,     0.11125,     0.89208,     0.20561,     0.58376,     0.15076,     0.10524,    0.014072,    0.019199,    0.004238], dtype=float32)),\n",
       " (array([     824.83,        97.3,      1276.2,      714.36], dtype=float32),\n",
       "  array([[      961.7,      238.58],\n",
       "         [     984.38,      213.57],\n",
       "         [     963.24,       217.8],\n",
       "         [     1067.4,      206.62],\n",
       "         [     989.05,         217],\n",
       "         [     1201.2,      308.85],\n",
       "         [     955.43,      338.89],\n",
       "         [     1253.8,      525.36],\n",
       "         [     907.83,       528.8],\n",
       "         [     1124.5,      667.94],\n",
       "         [     964.69,      386.65],\n",
       "         [     1149.1,      643.45],\n",
       "         [     983.97,      633.69],\n",
       "         [     1071.5,       705.3],\n",
       "         [     847.01,      675.39],\n",
       "         [     976.83,         720],\n",
       "         [     857.51,      675.77]], dtype=float32),\n",
       "  array([    0.94297,     0.92414,     0.58118,      0.8831,    0.076561,     0.97706,     0.97036,     0.92578,     0.91196,     0.89711,     0.87428,     0.85329,     0.85269,     0.18381,     0.19161,    0.022047,    0.024505], dtype=float32))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 3. Initialize IDs 0 and 1 from that first valid frame (by largest area)\n",
    "\n",
    "first_frame_dets = raw_estimators[first_valid_idx]\n",
    "\n",
    "\n",
    "# Sort by area so the main subjects get IDs 0 and 1 - the largest two bboxes \n",
    "first_sorted = sorted(first_frame_dets, key=lambda d: bbox_area(d[0]) if d[0] is not None else 0, reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "# Assign to tracks - getting the condinate of the first frame identified of the 2 peson and STORE by SIZE\n",
    "# arranging the first_sorted(which is raw_estimetors) so that the \"0\" would be the one how is the larger box in the \n",
    "#first frame identified with people\n",
    "\n",
    "# so here we have the keypoints of the first frame identified of the 2 peson and STORE by SIZE\n",
    "for pid in [0, 1]:\n",
    "    if pid < len(first_sorted):\n",
    "        bbox, kxy, kcf = first_sorted[pid]\n",
    "        tracks[pid][\"kpts_xy\"][first_valid_idx] = kxy\n",
    "        tracks[pid][\"kpts_conf\"][first_valid_idx] = kcf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac9fb6",
   "metadata": {},
   "source": [
    "Now with the heavy lifting; It plays the video forward and ensures that \"Person 0\" in Frame 10 remains \"Person 0\" in Frame 11, even if they move.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a0d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking done (dyad IDs: 0 and 1).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Main Tracking Loop\n",
    "\n",
    "for t in range(first_valid_idx + 1, n_frames): # It starts exactly one frame after your \"Casting\" frame and runs until the end of the video.\n",
    "    dets = raw_estimators[t]\n",
    "    if not dets:\n",
    "        continue\n",
    "\n",
    "    # Prepare previous centroids for matching\n",
    "    prev_centroids = []\n",
    "    prev_valid_pids = []\n",
    "\n",
    "    # The \"Memory\" Phase (Where were they?):\n",
    "    for pid in [0, 1]:\n",
    "        # Get last known position from the tracks we are building\n",
    "        last_kpts = tracks[pid][\"kpts_xy\"][t-1]\n",
    "        if not np.all(np.isnan(last_kpts)):\n",
    "            # Use the bounding box of the keypoints as the centroid\n",
    "            x1, y1 = np.nanmin(last_kpts[:, 0]), np.nanmin(last_kpts[:, 1])\n",
    "            x2, y2 = np.nanmax(last_kpts[:, 0]), np.nanmax(last_kpts[:, 1])\n",
    "            prev_centroids.append(np.array([(x1+x2)/2, (y1+y2)/2]))\n",
    "            prev_valid_pids.append(pid)\n",
    "\n",
    "    # Prepare current detections\n",
    "    curr_bboxes = [d[0] for d in dets if d[0] is not None]\n",
    "    curr_centroids = [bbox_centroid(b) for b in curr_bboxes]\n",
    "\n",
    "    if not curr_centroids or not prev_centroids:\n",
    "        # Fallback: if matching fails, just assign first available detections\n",
    "        for pid in [0, 1]:\n",
    "            if pid < len(dets):\n",
    "                _, kxy, kcf = dets[pid]\n",
    "                tracks[pid][\"kpts_xy\"][t] = kxy\n",
    "                tracks[pid][\"kpts_conf\"][t] = kcf\n",
    "        continue\n",
    "\n",
    "    # Match previous frame people to current frame people\n",
    "    mapping = greedy_assign(prev_centroids, curr_centroids)\n",
    "\n",
    "    used_det_indices = set()\n",
    "    for i, j in enumerate(mapping):\n",
    "        if j is None: continue\n",
    "        \n",
    "        pid = prev_valid_pids[i]\n",
    "        target_bbox = curr_bboxes[j]\n",
    "        \n",
    "        # Find which detection index in 'dets' this belongs to\n",
    "        for di, d in enumerate(dets):\n",
    "            if d[0] is not None and np.allclose(d[0], target_bbox) and di not in used_det_indices:\n",
    "                used_det_indices.add(di)\n",
    "                _, kxy, kcf = d\n",
    "                tracks[pid][\"kpts_xy\"][t] = kxy\n",
    "                tracks[pid][\"kpts_conf\"][t] = kcf\n",
    "                break\n",
    "\n",
    "print(\"Tracking done (dyad IDs: 0 and 1).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461eac5",
   "metadata": {},
   "source": [
    "now we have  tracks object is now a dictionary containing two main matrices for each person - keypooint and confidance.\n",
    "If my video is 30 seconds long at 30 FPS, you have 900 rows of data\n",
    "\n",
    "kpts_xy:\n",
    "    Rows: Frame Number (Time).\n",
    "    Columns: Body Part (Nose, Left Eye, Right Shoulder, etc.).\n",
    "    The values are the coordinates $(x, y)$.\n",
    "kpts_conf:\n",
    "    Rows: Frame Number (Time).\n",
    "    Columns: Body Part (Nose, Left Eye, Right Shoulder, etc.).\n",
    "    the value is a single number (0.0 to 1.0) representing how \"confident\" the model is\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798701ee",
   "metadata": {},
   "source": [
    "## Preprocessing- data handling and noise reduction \n",
    "Now lets deal with noise and uncertainty \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for cleaning keypoints\n",
    "def apply_conf_mask(kpts_xy: np.ndarray, kpts_conf: np.ndarray, conf_min: float = 0.3) -> np.ndarray:\n",
    "    \"\"\"Set (x,y) to NaN where confidence < conf_min.\"\"\"\n",
    "    out = kpts_xy.copy()\n",
    "    bad = (kpts_conf < conf_min) | np.isnan(kpts_conf)\n",
    "    out[bad, :] = np.nan\n",
    "    return out\n",
    "\n",
    "def interp_nan_1d(y: np.ndarray, max_gap: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Interpolate NaNs in a 1D array; only fill gaps <= max_gap.\n",
    "    \"\"\"\n",
    "    y2 = y.copy()\n",
    "    n = len(y2)\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    nan_mask = np.isnan(y2)\n",
    "    if nan_mask.all():\n",
    "        return y2\n",
    "\n",
    "    # Interpolate all NaNs first\n",
    "    y2[nan_mask] = np.interp(idx[nan_mask], idx[~nan_mask], y2[~nan_mask])\n",
    "\n",
    "    # Re-mask large gaps\n",
    "    # Find consecutive NaN runs in original\n",
    "    runs = []\n",
    "    start = None\n",
    "    for i in range(n):\n",
    "        if nan_mask[i] and start is None:\n",
    "            start = i\n",
    "        if (not nan_mask[i]) and start is not None:\n",
    "            runs.append((start, i-1))\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        runs.append((start, n-1))\n",
    "\n",
    "    for a, b in runs:\n",
    "        if (b - a + 1) > max_gap:\n",
    "            y2[a:b+1] = np.nan\n",
    "\n",
    "    return y2\n",
    "\n",
    "def smooth_1d(y: np.ndarray, window: int = 11, poly: int = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Savitzky-Golay smoothing, safe with NaNs (smooth only the valid part).\n",
    "    \"\"\"\n",
    "    y2 = y.copy()\n",
    "    valid = ~np.isnan(y2)\n",
    "    if valid.sum() < window:\n",
    "        return y2\n",
    "    y2[valid] = savgol_filter(y2[valid], window_length=window, polyorder=poly)\n",
    "    return y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb505214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean each person, each joint, both x and y\n",
    "CONF_THER= 0.3\n",
    "MAX_GAP_FRAMES = int(0.3 * fps)  # fill up to ~300ms gaps\n",
    "\n",
    "clean_xy = {}\n",
    "for pid in [0, 1]:\n",
    "    xy = tracks[pid][\"kpts_xy\"]          # (T,K,2)\n",
    "    cf = tracks[pid][\"kpts_conf\"]        # (T,K)\n",
    "\n",
    "    # Apply confidence mask frame-by-frame\n",
    "    xy_masked = np.full_like(xy, np.nan)\n",
    "    for t in range(n_frames):\n",
    "        xy_masked[t] = apply_conf_mask(xy[t], cf[t], conf_min=CONF_THER)\n",
    "\n",
    "    # Interpolate + smooth per joint, per coordinate over time\n",
    "    xy_filt = xy_masked.copy()\n",
    "    for j in range(K):\n",
    "        for c in [0, 1]:  # x,y\n",
    "            series = xy_masked[:, j, c]\n",
    "            series = interp_nan_1d(series, max_gap=MAX_GAP_FRAMES)\n",
    "            series = smooth_1d(series, window=11, poly=2)\n",
    "            xy_filt[:, j, c] = series\n",
    "\n",
    "    clean_xy[pid] = xy_filt\n",
    "\n",
    "print(\"Keypoints cleaned (confidence mask + interpolation + smoothing).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
