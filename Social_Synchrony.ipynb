{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9a12d6",
   "metadata": {},
   "source": [
    "### YOLO modeling\n",
    "\n",
    "after testing the fealability of using pose estimators I now move to a newer model one that can support video anylsis better then the MPI heatmap-appraoch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94156714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NumPy: 1.26.4\n",
      "âœ… OpenCV: 4.10.0\n",
      "ðŸš€ GPU Active: False\n",
      "8.3.241\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ultralytics\n",
    "\n",
    "print(f\"âœ… NumPy: {np.__version__}\")       # Expect 1.26.x\n",
    "print(f\"âœ… OpenCV: {cv2.__version__}\")     # Expect 4.10.x\n",
    "print(f\"ðŸš€ GPU Active: {torch.backends.mps.is_available()}\")\n",
    "print(ultralytics.__version__)                # Expect 8.2.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202555c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers \n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "def show_image(img_or_list: Union[np.ndarray, List[np.ndarray]], row_plot: int = 1, titles: List[str] = None):\n",
    "    \"\"\"\n",
    "    Display one or multiple images in a grid.\n",
    "    - img_or_list: single array or list of arrays\n",
    "    - row_plot: number of rows\n",
    "    - titles: optional list of titles (same length as images)\n",
    "    \"\"\"\n",
    "    imgs = img_or_list if isinstance(img_or_list, list) else [img_or_list]\n",
    "    n = len(imgs)\n",
    "    cols = int(np.ceil(n / row_plot))\n",
    "    rows = row_plot\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    axes = np.array(axes).reshape(-1) if isinstance(axes, np.ndarray) else np.array([axes])\n",
    "\n",
    "    for i, im in enumerate(imgs):\n",
    "        if im.ndim == 2:\n",
    "            axes[i].imshow(im, cmap='gray')\n",
    "        else:\n",
    "            axes[i].imshow(im)\n",
    "        axes[i].axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            axes[i].set_title(titles[i])\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c3b08",
   "metadata": {},
   "source": [
    "### Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f47e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt to 'yolo11n-pose.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.0MB 7.9MB/s 0.8s0.7s<0.1s.2ss\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO # type: ignore\n",
    "\n",
    "# 1. Load the model (YOLO11 is the 2025 standard)\n",
    "model = YOLO('yolo11n-pose.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f07b0a",
   "metadata": {},
   "source": [
    "# Social Synchrony\n",
    "The first metric I would examine would be joint velocity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2357a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Signal processing + stats\n",
    "from scipy.signal import savgol_filter, correlate\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from ultralytics import YOLO # type: ignore\n",
    "\n",
    "# -----------------------------\n",
    "# User config \n",
    "# -----------------------------\n",
    "VIDEO_PATH = \"./images/dancing.mp4\"     # <-- change\n",
    "MODEL_WEIGHTS = \"yolov8n-pose.pt\" # or yolov8s-pose.pt for stronger results\n",
    "CONF_THRES = 0.25\n",
    "IOU_THRES  = 0.5\n",
    "\n",
    "# Rolling window for synchrony (in seconds)\n",
    "ROLL_WIN_SEC = 2.0\n",
    "\n",
    "# Max lag for cross-correlation (in seconds)\n",
    "MAX_LAG_SEC = 1.0\n",
    "\n",
    "# Joint groups (COCO-17 indices; adjust if your model differs)\n",
    "# COCO keypoints order (common): \n",
    "# 0 nose, \n",
    "# 1 l_eye, \n",
    "# 2 r_eye, \n",
    "# 3 l_ear, \n",
    "# 4 r_ear,\n",
    "# 5 l_shoulder,\n",
    "#  6 r_shoulder,\n",
    "#  7 l_elbow, \n",
    "# 8 r_elbow, \n",
    "# 9  l_wrist, \n",
    "# 10 r_wrist,\n",
    "# 11 l_hip,\n",
    "#  12 r_hip, \n",
    "# 13 l_knee, \n",
    "# 14 r_knee, \n",
    "# 15 l_ankle, \n",
    "# 16 r_ankle\n",
    "UPPER_BODY = [5, 6, 7, 8, 9, 10]  # shoulders, elbows, wrists\n",
    "TORSO      = [11, 12, 5, 6]       # hips + shoulders\n",
    "LOWER_BODY = [13, 14, 15, 16]     # knees, ankles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781f7b9",
   "metadata": {},
   "source": [
    "### Loading the video \n",
    "\n",
    "since I want to asses synchrony I have you know the fps of the video processer in order for me to accurantly define what I consider to be a synchrenize movement. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "465e30fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS=29.970  dt=0.0334s\n"
     ]
    }
   ],
   "source": [
    "# --- Chunk 1: Video I/O + FPS ---\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open video.\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps is None or fps <= 1e-6:\n",
    "    # Fallback if metadata missing\n",
    "    fps = 30.0\n",
    "\n",
    "dt = 1.0 / fps\n",
    "print(f\"FPS={fps:.3f}  dt={dt:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef807ee7",
   "metadata": {},
   "source": [
    "The videoCapture function process around 30 frames per second, and alternativley every frame is 0.333 second interval from the previous one. \n",
    "\n",
    "Next, we would prcess the video through the YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65a23516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1690 frames of estimators.\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(MODEL_WEIGHTS)\n",
    "\n",
    "\n",
    "output_path = \"./data_output/output_skeleton2.mp4\"\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # type: ignore\n",
    "\n",
    "\n",
    "# We'll store raw estimators per frame in a list.\n",
    "# Each item: a list of estimators, where estimator = (bbox_xyxy, kpts_xy, kpts_conf)\n",
    "raw_estimators: List[List[Tuple[np.ndarray, np.ndarray, np.ndarray]]] = []\n",
    "\n",
    "\n",
    "# Stuff for Video I/O\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps is None or fps <= 0:\n",
    "    fps = 30.0  # fallback (important!)\n",
    "frame_idx = 0\n",
    "    # add a video writer to fetch the skelaton video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# starting the every frame processing: \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "    # Run pose prediction\n",
    "    # Note: model(frame) returns a Results list (usually len 1 for one image)\n",
    "\n",
    "    # before passing the frame into the model I want to have a filter to improve the quality:\n",
    "    blurred_frame = cv2.GaussianBlur(frame, (7, 7), 1.0)\n",
    "    high_freq = cv2.subtract(frame.astype(float), blurred_frame.astype(float))\n",
    "    alpha = 1.5\n",
    "    sharpened = frame.astype(float) + alpha * high_freq\n",
    "    sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "        \n",
    "    results = model.predict(\n",
    "        source=sharpened,\n",
    "        conf=CONF_THRES,\n",
    "        iou=IOU_THRES,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    dets_this_frame = []\n",
    "    r0 = results[0]\n",
    "\n",
    "    # r0.boxes.xyxy -> Nx4\n",
    "    # r0.keypoints.xy -> NxKx2\n",
    "    # r0.keypoints.conf -> NxK\n",
    "    if r0.keypoints is not None and len(r0.keypoints) > 0:\n",
    "        boxes_xyxy = r0.boxes.xyxy.cpu().numpy() if r0.boxes is not None else None # type: ignore\n",
    "\n",
    "        kpts_xy    = r0.keypoints.xy.cpu().numpy() # type: ignore\n",
    "\n",
    "        kpts_conf  = r0.keypoints.conf.cpu().numpy()  # type: ignore\n",
    "\n",
    "        n_people = kpts_xy.shape[0]\n",
    "        for i in range(n_people):\n",
    "            bbox = boxes_xyxy[i] if boxes_xyxy is not None else None\n",
    "            dets_this_frame.append((bbox, kpts_xy[i], kpts_conf[i]))\n",
    "\n",
    "    raw_estimators.append(dets_this_frame)\n",
    "\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    if annotated_frame.shape[:2] != (frame_height, frame_width):\n",
    "        annotated_frame = cv2.resize(annotated_frame, (frame_width, frame_height))\n",
    "    if annotated_frame.dtype != np.uint8:\n",
    "        annotated_frame = annotated_frame.astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    # write a video frame with skeleton\n",
    "\n",
    "    out.write(annotated_frame)\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "n_frames = len(raw_estimators)\n",
    "times = np.arange(n_frames) * dt\n",
    "print(f\"Loaded {n_frames} frames of estimators.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e9bd7",
   "metadata": {},
   "source": [
    "The next section is needed only if the video contains 2 people. In my case I know that the SRL labs have 3 videos for each interactione ( 1 video containing the both side of the dyad while the other 2 videos are capturing each individual seperately). therefore, the next chulk might come handy if i would use only the dyad video - although that I intend to use all 3 videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02bd07c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No detections in first frame. Try a clearer video or lower CONF_THRES.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m first = raw_estimators[\u001b[32m0\u001b[39m]\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo detections in first frame. Try a clearer video or lower CONF_THRES.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Sort by area (descending) if bbox exists, otherwise keep order\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: No detections in first frame. Try a clearer video or lower CONF_THRES."
     ]
    }
   ],
   "source": [
    "# --- Chunk 3: Identity tracking for dyads (centroid continuity) ---\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def bbox_centroid(bbox_xyxy: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute centroid from [x1,y1,x2,y2].\"\"\"\n",
    "    x1, y1, x2, y2 = bbox_xyxy\n",
    "    return np.array([(x1 + x2) / 2.0, (y1 + y2) / 2.0], dtype=float)\n",
    "\n",
    "def bbox_area(bbox_xyxy: np.ndarray) -> float:\n",
    "    \"\"\"Compute area of bounding box.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox_xyxy\n",
    "    return max(0.0, x2 - x1) * max(0.0, y2 - y1)\n",
    "\n",
    "def greedy_assign(prev_centroids: List[np.ndarray],\n",
    "                  curr_centroids: List[np.ndarray]) -> List[Optional[int]]:\n",
    "    \"\"\"\n",
    "    Greedy assignment: returns mapping from prev index -> curr index (or None).\n",
    "    \"\"\"\n",
    "    if len(prev_centroids) == 0 or len(curr_centroids) == 0:\n",
    "        return [None] * len(prev_centroids)\n",
    "\n",
    "    # Distance matrix (prev x curr)\n",
    "    D = np.zeros((len(prev_centroids), len(curr_centroids)), dtype=float)\n",
    "    for i, pc in enumerate(prev_centroids):\n",
    "        for j, cc in enumerate(curr_centroids):\n",
    "            D[i, j] = np.linalg.norm(pc - cc)\n",
    "\n",
    "    assigned_curr = set()\n",
    "    mapping = [None] * len(prev_centroids)\n",
    "\n",
    "    for _ in range(min(len(prev_centroids), len(curr_centroids))):\n",
    "        # Find the global minimum distance remaining in D\n",
    "        i, j = np.unravel_index(np.argmin(D), D.shape)\n",
    "        if D[i, j] == np.inf:\n",
    "            break\n",
    "            \n",
    "        mapping[i] = j\n",
    "        assigned_curr.add(j)\n",
    "        # \"Remove\" this row and column from future consideration\n",
    "        D[i, :] = np.inf\n",
    "        D[:, j] = np.inf\n",
    "\n",
    "    return mapping\n",
    "\n",
    "# 1. Initialize data containers\n",
    "K = 17  # Standard COCO keypoints\n",
    "n_frames = len(raw_estimators)\n",
    "tracks = {\n",
    "    0: {\"kpts_xy\": np.full((n_frames, K, 2), np.nan, dtype=float),\n",
    "        \"kpts_conf\": np.full((n_frames, K), np.nan, dtype=float)},\n",
    "    1: {\"kpts_xy\": np.full((n_frames, K, 2), np.nan, dtype=float),\n",
    "        \"kpts_conf\": np.full((n_frames, K), np.nan, dtype=float)},\n",
    "}\n",
    "\n",
    "# 2. Find the FIRST frame that actually contains a detection\n",
    "first_valid_idx = None\n",
    "for i, frame_dets in enumerate(raw_estimators):\n",
    "    if len(frame_dets) > 0:\n",
    "        first_valid_idx = i\n",
    "        break\n",
    "\n",
    "if first_valid_idx is None:\n",
    "    raise RuntimeError(\"No detections found in any frame. Check your video or CONF_THRES.\")\n",
    "\n",
    "print(f\"Starting tracking from frame index: {first_valid_idx}\")\n",
    "\n",
    "# 3. Initialize IDs 0 and 1 from that first valid frame (by largest area)\n",
    "first_frame_dets = raw_estimators[first_valid_idx]\n",
    "# Sort by area so the main subjects get IDs 0 and 1\n",
    "first_sorted = sorted(first_frame_dets, key=lambda d: bbox_area(d[0]) if d[0] is not None else 0, reverse=True)\n",
    "\n",
    "for pid in [0, 1]:\n",
    "    if pid < len(first_sorted):\n",
    "        bbox, kxy, kcf = first_sorted[pid]\n",
    "        tracks[pid][\"kpts_xy\"][first_valid_idx] = kxy\n",
    "        tracks[pid][\"kpts_conf\"][first_valid_idx] = kcf\n",
    "\n",
    "# 4. Main Tracking Loop\n",
    "for t in range(first_valid_idx + 1, n_frames):\n",
    "    dets = raw_estimators[t]\n",
    "    if not dets:\n",
    "        continue\n",
    "\n",
    "    # Prepare previous centroids for matching\n",
    "    prev_centroids = []\n",
    "    prev_valid_pids = []\n",
    "    for pid in [0, 1]:\n",
    "        # Get last known position from the tracks we are building\n",
    "        last_kpts = tracks[pid][\"kpts_xy\"][t-1]\n",
    "        if not np.all(np.isnan(last_kpts)):\n",
    "            # Use the bounding box of the keypoints as the centroid\n",
    "            x1, y1 = np.nanmin(last_kpts[:, 0]), np.nanmin(last_kpts[:, 1])\n",
    "            x2, y2 = np.nanmax(last_kpts[:, 0]), np.nanmax(last_kpts[:, 1])\n",
    "            prev_centroids.append(np.array([(x1+x2)/2, (y1+y2)/2]))\n",
    "            prev_valid_pids.append(pid)\n",
    "\n",
    "    # Prepare current detections\n",
    "    curr_bboxes = [d[0] for d in dets if d[0] is not None]\n",
    "    curr_centroids = [bbox_centroid(b) for b in curr_bboxes]\n",
    "\n",
    "    if not curr_centroids or not prev_centroids:\n",
    "        # Fallback: if matching fails, just assign first available detections\n",
    "        for pid in [0, 1]:\n",
    "            if pid < len(dets):\n",
    "                _, kxy, kcf = dets[pid]\n",
    "                tracks[pid][\"kpts_xy\"][t] = kxy\n",
    "                tracks[pid][\"kpts_conf\"][t] = kcf\n",
    "        continue\n",
    "\n",
    "    # Match previous frame people to current frame people\n",
    "    mapping = greedy_assign(prev_centroids, curr_centroids)\n",
    "\n",
    "    used_det_indices = set()\n",
    "    for i, j in enumerate(mapping):\n",
    "        if j is None: continue\n",
    "        \n",
    "        pid = prev_valid_pids[i]\n",
    "        target_bbox = curr_bboxes[j]\n",
    "        \n",
    "        # Find which detection index in 'dets' this belongs to\n",
    "        for di, d in enumerate(dets):\n",
    "            if d[0] is not None and np.allclose(d[0], target_bbox) and di not in used_det_indices:\n",
    "                used_det_indices.add(di)\n",
    "                _, kxy, kcf = d\n",
    "                tracks[pid][\"kpts_xy\"][t] = kxy\n",
    "                tracks[pid][\"kpts_conf\"][t] = kcf\n",
    "                break\n",
    "\n",
    "print(\"Tracking done (dyad IDs: 0 and 1).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
