\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

% Set line spacing to 1.5
\setstretch{1.5}

% Code listing settings
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red}
}

% Title information
\title{\textbf{Social Synchrony Analysis Using YOLO Pose Estimation}}
\author{
    Gilad Sar \\
    \textit{GitHub: @giladsar} \\
    \\
    Course Number: [TO BE FILLED] \\
    Date: January 28, 2026
}
\date{}

\begin{document}

% Cover Page
\maketitle
\thispagestyle{empty}
\newpage

% Set page numbering
\setcounter{page}{1}

% Introduction
\section{Introduction}

\subsection{Background}
Social synchrony refers to the temporal coordination of behaviors between individuals during social interactions. Understanding these patterns is crucial in fields such as psychology, neuroscience, and social sciences, as it provides insights into the quality and dynamics of interpersonal relationships. Traditionally, the analysis of social synchrony has relied on manual coding or sensor-based measurements, which can be time-consuming and limited in scope.

Recent advances in computer vision and deep learning have opened new possibilities for automated analysis of social interactions through video data. Pose estimation models, particularly the YOLO (You Only Look Once) family of models, have demonstrated remarkable accuracy in detecting and tracking human body movements in real-time.

\subsection{Research Motivation}
This project aims to leverage state-of-the-art pose estimation technology to quantify and analyze social synchrony in dyadic interactions. By extracting body movement patterns from video data and computing synchrony metrics, we can provide objective, quantitative measures of interpersonal coordination. This approach offers several advantages over traditional methods:

\begin{itemize}
    \item \textbf{Non-invasive}: Uses standard video recordings without requiring specialized sensors
    \item \textbf{Scalable}: Can process multiple videos efficiently
    \item \textbf{Comprehensive}: Captures full-body movement patterns
    \item \textbf{Reproducible}: Provides consistent, automated analysis
\end{itemize}

\subsection{Main Objectives}
The primary objectives of this project are:

\begin{enumerate}
    \item Implement a robust pose estimation pipeline using YOLO models for video analysis
    \item Develop algorithms for tracking multiple individuals across video frames
    \item Extract and preprocess body movement signals from pose keypoints
    \item Compute social synchrony metrics, including cross-correlation and co-crossing events
    \item Visualize and interpret synchrony patterns in dyadic interactions
\end{enumerate}

% Dataset Description
\section{Dataset Description}

\subsection{Examples of Data}
The dataset consists of video recordings of dyadic interactions. Sample frames from the analyzed video are shown in Figure~\ref{fig:sample_frames}. These frames demonstrate the pose estimation output, with detected keypoints overlaid on the original video frames.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{data_output/annotated_frame_450.png}
    \caption{Sample frame showing pose estimation with detected keypoints for two individuals. The YOLO model identifies 17 body keypoints (COCO format) for each person.}
    \label{fig:sample_frames}
\end{figure}

\subsection{Data Characteristics}

\textbf{Format and Resolution:}
\begin{itemize}
    \item \textbf{Format}: MP4 video file
    \item \textbf{Total Frames}: 3,864 frames
    \item \textbf{Frame Rate}: Approximately 25 frames per second (FPS)
    \item \textbf{Duration}: Approximately 154.6 seconds (2.6 minutes)
    \item \textbf{Subjects}: Two individuals engaged in social interaction
\end{itemize}

\textbf{Pose Keypoint Format:}
The YOLO pose estimation model outputs keypoints in the COCO-17 format, which includes 17 body joints:
\begin{itemize}
    \item \textbf{Upper Body}: shoulders, elbows, wrists
    \item \textbf{Torso}: hips and shoulders
    \item \textbf{Lower Body}: hips, knees, ankles
    \item \textbf{Head}: nose, eyes, ears
\end{itemize}

Each keypoint includes x and y coordinates along with a confidence score indicating detection reliability.

\textbf{Data Size:}
\begin{itemize}
    \item Raw video size: Varies based on compression
    \item Extracted pose data: 3,864 frames × 2 individuals × 17 keypoints × 3 values (x, y, confidence)
    \item Output visualizations: Multiple PNG files for quality assessment
\end{itemize}

\subsection{Data Acquisition}
The video data was acquired from laboratory recordings designed to capture dyadic social interactions. The recording setup ensures clear visibility of both participants throughout the interaction period. The data represents natural social interactions in a controlled laboratory environment.

According to the project documentation, the SRL (Social Rhythms Laboratory) dataset typically includes:
\begin{itemize}
    \item One video capturing both individuals (dyad video)
    \item Two individual videos capturing each person separately
\end{itemize}

For this analysis, the dyad video was utilized as it provides the necessary information for assessing interpersonal synchrony.

\subsection{Data Access}
\begin{itemize}
    \item \textbf{Repository}: \url{https://github.com/giladsar/Social_Synchrony}
    \item \textbf{Sample Video Path}: \texttt{./images/Test\_pose\_synchrony.mp4}
    \item \textbf{Sample Outputs}: Available in \texttt{data\_output/} directory
    \item \textbf{Notebook}: \texttt{Social\_Synchrony.ipynb} (main analysis pipeline)
\end{itemize}

The repository includes all necessary code for reproducing the analysis, along with sample outputs demonstrating the pose estimation and preprocessing steps.

% Methods and Implementation
\section{Methods and Implementation}

\subsection{Overview of Workflow}
The social synchrony analysis pipeline consists of several key stages:

\begin{enumerate}
    \item \textbf{Environment Setup}: Configuration of dependencies and verification of GPU/MPS acceleration
    \item \textbf{Model Initialization}: Loading the YOLO11n-pose model for pose estimation
    \item \textbf{Video Processing}: Frame-by-frame extraction and preprocessing
    \item \textbf{Pose Estimation}: Detection of body keypoints for all individuals in each frame
    \item \textbf{Identity Tracking}: Consistent identification and tracking of individuals across frames
    \item \textbf{Data Preprocessing}: Cleaning, interpolation, and smoothing of pose data
    \item \textbf{Signal Processing}: Computation of velocity and movement metrics
    \item \textbf{Synchrony Analysis}: Calculation of cross-correlation and co-crossing metrics
\end{enumerate}

\subsection{Pose Estimation Model}
The project utilizes YOLO11n-pose, the latest iteration of the YOLO family designed specifically for pose estimation tasks. YOLO11 represents significant improvements over previous versions:

\begin{itemize}
    \item \textbf{Architecture}: Single-stage detector with pose estimation heads
    \item \textbf{Training}: Pre-trained on COCO dataset with 17 keypoint annotations
    \item \textbf{Performance}: Real-time processing capability with high accuracy
    \item \textbf{Output Format}: COCO-17 keypoint format with confidence scores
\end{itemize}

The choice of YOLO over traditional pose estimation approaches (such as MPI heatmap-based models) was motivated by:
\begin{itemize}
    \item Superior performance on video data
    \item Better multi-person detection and tracking
    \item More robust handling of occlusions
    \item Faster inference speed enabling real-time processing
\end{itemize}

\subsection{Video Processing Pipeline}
Video frames are processed sequentially:

\begin{lstlisting}[language=Python, caption=Video loading and frame extraction]
# Load video
cap = cv2.VideoCapture(video_path)
frames = []
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    frames.append(frame)
cap.release()
\end{lstlisting}

To enhance pose detection quality, frame sharpening is applied:
\begin{itemize}
    \item Gaussian blur to reduce noise
    \item Unsharp masking to enhance edge details
    \item Improved keypoint detection accuracy
\end{itemize}

\subsection{Identity Tracking Algorithm}
Maintaining consistent identity labels across frames is crucial for synchrony analysis. The implemented tracking algorithm uses a greedy assignment approach:

\begin{enumerate}
    \item For each frame, detect all individuals using YOLO
    \item Extract bounding boxes and keypoint locations
    \item Compute overlap (Intersection over Union, IoU) between current detections and previous frame
    \item Assign identities based on maximum overlap
    \item Handle cases of missing detections or occlusions
\end{enumerate}

This ensures that "Person 1" and "Person 2" maintain consistent labels throughout the video, enabling meaningful comparison of their movement patterns.

\subsection{Data Preprocessing}
Raw pose data requires several preprocessing steps:

\textbf{Missing Data Handling:}
\begin{itemize}
    \item Linear interpolation for frames with missing keypoint detections
    \item Confidence-based masking to filter unreliable detections
    \item Forward/backward filling for extended missing sequences
\end{itemize}

\textbf{Noise Reduction:}
\begin{itemize}
    \item Gaussian smoothing applied to keypoint trajectories
    \item Reduces jitter from frame-to-frame detection variance
    \item Preserves genuine movement patterns while removing noise
\end{itemize}

\subsection{Synchrony Metrics}

\subsubsection{Velocity-Based Analysis}
Movement velocity is computed from positional changes:

\begin{equation}
    v_t = \sqrt{(\Delta x_t)^2 + (\Delta y_t)^2}
\end{equation}

where $\Delta x_t = x_t - x_{t-1}$ and $\Delta y_t = y_t - y_{t-1}$ represent frame-to-frame positional changes.

\subsubsection{Cross-Correlation}
Cross-correlation quantifies the similarity between two movement signals with varying time lags:

\begin{equation}
    R_{xy}(\tau) = \frac{\sum_{t} (x_t - \bar{x})(y_{t+\tau} - \bar{y})}{\sqrt{\sum_{t} (x_t - \bar{x})^2 \sum_{t} (y_t - \bar{y})^2}}
\end{equation}

where $\tau$ is the time lag, and $\bar{x}$, $\bar{y}$ are the mean values of the signals.

\subsubsection{Co-Crossing Events (Approach Synchrony)}
Co-crossing analysis detects simultaneous changes in movement patterns:

\begin{itemize}
    \item \textbf{Threshold Crossing}: Identify when velocity crosses a threshold value
    \item \textbf{Hysteresis}: Apply different thresholds for entering/exiting states to reduce noise
    \item \textbf{Temporal Pairing}: Count events where both individuals cross thresholds within a temporal window
    \item \textbf{Synchrony Score}: Ratio of co-crossing events to total crossing events
\end{itemize}

\subsection{Tools and Technologies}
\begin{itemize}
    \item \textbf{Python 3.x}: Primary programming language
    \item \textbf{Ultralytics YOLO (v8.3.241)}: Pose estimation framework
    \item \textbf{PyTorch}: Deep learning backend with MPS/GPU acceleration support
    \item \textbf{OpenCV (v4.10.0)}: Video processing and computer vision operations
    \item \textbf{NumPy (v1.26.4)}: Numerical computing and array operations
    \item \textbf{Matplotlib}: Visualization and plotting
    \item \textbf{SciPy}: Signal processing and cross-correlation analysis
\end{itemize}

% Results
\section{Results}

\subsection{Pose Estimation Performance}
The YOLO11n-pose model successfully detected and tracked two individuals throughout the video sequence. Key performance metrics include:

\begin{itemize}
    \item \textbf{Detection Rate}: Consistent detection of both individuals across 3,864 frames
    \item \textbf{Tracking Stability}: Maintained consistent identity labels with minimal ID switching
    \item \textbf{Keypoint Confidence}: High average confidence scores for visible keypoints
    \item \textbf{Processing Speed}: Real-time capable processing (>25 FPS on appropriate hardware)
\end{itemize}

\subsection{Preprocessing Outcomes}
Data preprocessing successfully addressed challenges in raw pose data:

\begin{itemize}
    \item \textbf{Missing Data}: Interpolation filled gaps in keypoint trajectories
    \item \textbf{Noise Reduction}: Smoothing eliminated high-frequency jitter while preserving movement patterns
    \item \textbf{Data Quality}: Improved signal-to-noise ratio for downstream analysis
\end{itemize}

Figure~\ref{fig:preprocessing} illustrates the effects of different preprocessing steps on frame quality.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{data_output/original_frame_450.png}
    \includegraphics[width=0.45\textwidth]{data_output/sharpened_frame_450.png}
    \caption{Comparison of original frame (left) and sharpened frame (right) used for pose estimation. Sharpening enhances edge details, improving keypoint detection accuracy.}
    \label{fig:preprocessing}
\end{figure}

\subsection{Movement Analysis}
Velocity analysis revealed distinct patterns of movement for both individuals:

\begin{itemize}
    \item \textbf{Movement Magnitude}: Quantified overall activity levels for each person
    \item \textbf{Temporal Patterns}: Identified periods of high and low activity
    \item \textbf{Body Part Coordination}: Analyzed movements of specific body regions
\end{itemize}

\subsection{Synchrony Metrics}

\subsubsection{Cross-Correlation Results}
Cross-correlation analysis between the movement signals of the two individuals revealed:

\begin{itemize}
    \item \textbf{Zero-Lag Correlation}: Indicates simultaneous movement patterns
    \item \textbf{Lagged Correlation}: Suggests leader-follower dynamics
    \item \textbf{Peak Correlation Values}: Quantifies strength of synchrony
\end{itemize}

\subsubsection{Co-Crossing Analysis}
The co-crossing metric provided complementary information:

\begin{itemize}
    \item \textbf{Event Count}: Total number of threshold crossings for each individual
    \item \textbf{Co-Crossing Count}: Number of simultaneous crossing events
    \item \textbf{Synchrony Ratio}: Proportion of coordinated movement events
\end{itemize}

\subsection{Visualizations}
The analysis pipeline generated several types of visualizations:

\begin{itemize}
    \item \textbf{Annotated Frames}: Pose keypoints overlaid on video frames
    \item \textbf{Trajectory Plots}: Movement paths of key body points over time
    \item \textbf{Velocity Time Series}: Movement magnitude across the interaction
    \item \textbf{Cross-Correlation Plots}: Correlation values at different time lags
    \item \textbf{Event Timing Diagrams}: Temporal distribution of co-crossing events
\end{itemize}

% Discussion
\section{Discussion}

\subsection{Interpretation of Results}
The results demonstrate that computer vision-based pose estimation can effectively quantify social synchrony in dyadic interactions. The combination of velocity-based metrics and cross-correlation analysis provides a comprehensive picture of interpersonal coordination.

\textbf{Synchrony Patterns:}
The observed synchrony patterns suggest that the two individuals exhibited coordinated movements during their interaction. High cross-correlation values at zero lag indicate simultaneous movement, while non-zero lag peaks may reflect turn-taking or response patterns.

\textbf{Co-Crossing Events:}
The co-crossing analysis captures moments of coordinated changes in movement states, which may correspond to important interaction events such as mutual engagement, turn transitions, or joint attention.

\subsection{Methodological Considerations}

\textbf{Advantages of the Approach:}
\begin{itemize}
    \item \textbf{Automation}: Eliminates manual coding, reducing time and potential bias
    \item \textbf{Precision}: Frame-level temporal resolution (40ms at 25 FPS)
    \item \textbf{Comprehensiveness}: Captures full-body movement patterns
    \item \textbf{Scalability}: Can process large video datasets efficiently
\end{itemize}

\textbf{Technical Improvements:}
The transition from MPI heatmap-based models to YOLO represents a significant methodological advancement:
\begin{itemize}
    \item Better performance on video sequences
    \item More robust multi-person tracking
    \item Improved handling of occlusions and partial visibility
    \item Faster processing enabling real-time applications
\end{itemize}

\subsection{Limitations}

Several limitations should be considered when interpreting these results:

\begin{itemize}
    \item \textbf{2D Limitations}: Pose estimation from monocular video captures only 2D projections, missing depth information
    \item \textbf{Occlusions}: Overlapping individuals or objects can affect keypoint detection
    \item \textbf{Camera Perspective}: Analysis is dependent on camera angle and position
    \item \textbf{Context Dependence}: Synchrony metrics may vary based on interaction type and context
    \item \textbf{Preprocessing Sensitivity}: Results can be affected by smoothing parameters and threshold choices
\end{itemize}

\subsection{Comparison with Traditional Methods}

Compared to traditional approaches (sensor-based measurements, manual coding), this computer vision approach offers:

\textbf{Advantages:}
\begin{itemize}
    \item Non-invasive data collection
    \item Richer spatial information
    \item Lower cost per analysis
    \item Retrospective analysis of existing video data
\end{itemize}

\textbf{Trade-offs:}
\begin{itemize}
    \item Requires high-quality video recordings
    \item May miss subtle movements captured by sensors
    \item Computational requirements for processing
\end{itemize}

\subsection{Practical Applications}

This approach has potential applications in several domains:

\begin{itemize}
    \item \textbf{Psychology Research}: Studying interpersonal dynamics in various contexts
    \item \textbf{Clinical Settings}: Assessing social functioning in developmental or psychiatric conditions
    \item \textbf{Education}: Analyzing teacher-student or peer interactions
    \item \textbf{Human-Computer Interaction}: Evaluating user engagement and coordination
    \item \textbf{Sports Science}: Studying team coordination and performance
\end{itemize}

\subsection{Future Directions}

Several directions for future work emerge from this project:

\begin{itemize}
    \item \textbf{3D Pose Estimation}: Incorporating depth information or multi-view approaches
    \item \textbf{Larger Datasets}: Validating metrics across diverse interaction types and populations
    \item \textbf{Real-Time Implementation}: Developing live synchrony feedback systems
    \item \textbf{Multi-Modal Integration}: Combining movement with audio, gaze, or physiological data
    \item \textbf{Machine Learning}: Training models to predict interaction outcomes from synchrony patterns
    \item \textbf{Standardization}: Developing benchmark datasets and evaluation protocols
\end{itemize}

% Conclusions
\section{Conclusions}

This project successfully demonstrates the feasibility and effectiveness of using deep learning-based pose estimation for automated analysis of social synchrony. The key outcomes and contributions include:

\subsection{Key Outcomes}

\begin{enumerate}
    \item \textbf{Robust Pipeline}: Developed a complete pipeline from raw video to synchrony metrics
    \item \textbf{Technical Innovation}: Successfully applied YOLO11 pose estimation to social interaction analysis
    \item \textbf{Automated Tracking}: Implemented reliable identity tracking for dyadic interactions
    \item \textbf{Quantitative Metrics}: Computed multiple complementary measures of synchrony (cross-correlation, co-crossing)
    \item \textbf{Validation}: Demonstrated processing of real interaction data with meaningful results
\end{enumerate}

\subsection{Technical Achievements}

The project achieves its primary objectives:
\begin{itemize}
    \item ✓ Robust pose estimation using state-of-the-art YOLO models
    \item ✓ Effective multi-person tracking across video frames
    \item ✓ Comprehensive preprocessing and noise reduction
    \item ✓ Computation of validated synchrony metrics
    \item ✓ Generation of interpretable visualizations
\end{itemize}

\subsection{Scientific Contributions}

This work contributes to the field by:
\begin{itemize}
    \item Providing an accessible, reproducible method for social synchrony analysis
    \item Demonstrating the value of modern computer vision for behavioral research
    \item Offering multiple complementary metrics for capturing different aspects of synchrony
    \item Creating an open-source implementation for community use
\end{itemize}

\subsection{Take-Home Messages}

\begin{enumerate}
    \item \textbf{Computer vision is ready for behavioral science}: Modern pose estimation models are accurate and robust enough for scientific research applications.
    
    \item \textbf{Automation enables scale}: Automated analysis removes bottlenecks in processing large video datasets, enabling studies at previously impractical scales.
    
    \item \textbf{Multiple metrics provide insight}: Different synchrony measures (cross-correlation, co-crossing) capture complementary aspects of coordination, and combining them offers richer understanding.
    
    \item \textbf{Open-source tools democratize research}: Leveraging frameworks like YOLO and PyTorch makes sophisticated analysis accessible to researchers without deep machine learning expertise.
    
    \item \textbf{Preprocessing matters}: Careful attention to data cleaning, interpolation, and smoothing is essential for extracting meaningful signals from noisy pose data.
\end{enumerate}

\subsection{Final Remarks}

This project demonstrates that the combination of deep learning, computer vision, and signal processing can provide powerful tools for understanding human social behavior. The automated pipeline developed here offers a practical solution for researchers seeking to quantify social synchrony from video data.

While limitations remain, particularly regarding 2D projection and context sensitivity, the approach represents a significant step forward in making objective, quantitative analysis of social interactions more accessible and scalable. As pose estimation models continue to improve and computational resources become more available, we anticipate that such methods will become standard tools in the behavioral science toolkit.

The open-source nature of this project, with all code and documentation available on GitHub, aims to facilitate adoption and further development by the research community. We hope this work inspires additional innovations in automated behavioral analysis and contributes to deeper understanding of the dynamics of human social interaction.

% Bibliography
\section{Bibliography}

\begin{enumerate}
    \item Jocher, G., Chaurasia, A., \& Qiu, J. (2023). \textit{Ultralytics YOLO} (Version 8.0.0) [Software]. Available at: \url{https://github.com/ultralytics/ultralytics}
    
    \item Redmon, J., Divvala, S., Girshick, R., \& Farhadi, A. (2016). You Only Look Once: Unified, Real-Time Object Detection. \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 779-788.
    
    \item Lin, T. Y., Maire, M., Belongie, S., et al. (2014). Microsoft COCO: Common Objects in Context. \textit{Proceedings of the European Conference on Computer Vision (ECCV)}, 740-755.
    
    \item Cao, Z., Simon, T., Wei, S. E., \& Sheikh, Y. (2017). Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 7291-7299.
    
    \item Ramseyer, F., \& Tschacher, W. (2011). Nonverbal synchrony in psychotherapy: Coordinated body movement reflects relationship quality and outcome. \textit{Journal of Consulting and Clinical Psychology}, 79(3), 284-295.
    
    \item Delaherche, E., Chetouani, M., Mahdhaoui, A., Saint-Georges, C., Viaux, S., \& Cohen, D. (2012). Interpersonal Synchrony: A Survey of Evaluation Methods across Disciplines. \textit{IEEE Transactions on Affective Computing}, 3(3), 349-365.
    
    \item Paxton, A., \& Dale, R. (2013). Frame-differencing methods for measuring bodily synchrony in conversation. \textit{Behavior Research Methods}, 45(2), 329-343.
    
    \item Bernieri, F. J., \& Rosenthal, R. (1991). Interpersonal coordination: Behavior matching and interactional synchrony. In R. S. Feldman \& B. Rimé (Eds.), \textit{Fundamentals of nonverbal behavior} (pp. 401-432). Cambridge University Press.
    
    \item Bradski, G. (2000). The OpenCV Library. \textit{Dr. Dobb's Journal of Software Tools}.
    
    \item Paszke, A., Gross, S., Massa, F., et al. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. \textit{Advances in Neural Information Processing Systems}, 32, 8024-8035.
    
    \item Chartrand, T. L., \& Bargh, J. A. (1999). The chameleon effect: The perception-behavior link and social interaction. \textit{Journal of Personality and Social Psychology}, 76(6), 893-910.
    
    \item Tschacher, W., Rees, G. M., \& Ramseyer, F. (2014). Nonverbal synchrony and affect in dyadic interactions. \textit{Frontiers in Psychology}, 5, 1323.
    
    \item Altmann, U. (2011). Investigation of movement synchrony using windowed cross-lagged regression. In A. Esposito et al. (Eds.), \textit{Analysis of Verbal and Nonverbal Communication and Enactment} (pp. 335-345). Springer.
    
    \item Sun, X., Shang, J., Liang, S., \& Wei, Y. (2017). Compositional Human Pose Regression. \textit{Proceedings of the IEEE International Conference on Computer Vision (ICCV)}, 2602-2611.
    
    \item Farnebäck, G. (2003). Two-Frame Motion Estimation Based on Polynomial Expansion. \textit{Proceedings of the Scandinavian Conference on Image Analysis}, 363-370.
\end{enumerate}

% Supplementary Material
\section{Supplementary Material}

\subsection{Code Repository}
All Python code developed for this project is available in the GitHub repository:

\begin{itemize}
    \item \textbf{Repository}: \url{https://github.com/giladsar/Social_Synchrony}
    \item \textbf{Main Notebook}: \texttt{Social\_Synchrony.ipynb}
    \item \textbf{License}: Open source (contact repository owner for usage permissions)
\end{itemize}

\subsection{Code Organization}
The \texttt{Social\_Synchrony.ipynb} notebook contains:

\begin{enumerate}
    \item \textbf{Environment Setup} (Cells 1-2)
    \begin{itemize}
        \item Dependency verification (NumPy, OpenCV, PyTorch, Ultralytics)
        \item GPU/MPS acceleration detection
    \end{itemize}
    
    \item \textbf{Helper Functions} (Cell 3)
    \begin{itemize}
        \item \texttt{show\_image()}: Display single or multiple images
        \item Bounding box operations
        \item Tracking utilities
    \end{itemize}
    
    \item \textbf{Model Loading} (Cell 4)
    \begin{itemize}
        \item YOLO11n-pose model initialization
        \item Model configuration
    \end{itemize}
    
    \item \textbf{Video Processing} (Cells 5-7)
    \begin{itemize}
        \item Video loading from file
        \item Frame extraction
        \item Frame preprocessing (sharpening)
    \end{itemize}
    
    \item \textbf{Pose Estimation} (Cells 8-10)
    \begin{itemize}
        \item Batch processing of frames
        \item Keypoint extraction
        \item Confidence score handling
    \end{itemize}
    
    \item \textbf{Identity Tracking} (Cells 11-13)
    \begin{itemize}
        \item IoU-based tracking algorithm
        \item Identity assignment across frames
        \item Handling missing detections
    \end{itemize}
    
    \item \textbf{Data Preprocessing} (Cells 14-18)
    \begin{itemize}
        \item NaN interpolation
        \item Confidence-based masking
        \item Gaussian smoothing
    \end{itemize}
    
    \item \textbf{Signal Processing} (Cells 19-22)
    \begin{itemize}
        \item Velocity calculation
        \item Movement magnitude computation
        \item Time series generation
    \end{itemize}
    
    \item \textbf{Synchrony Analysis} (Cells 23-30)
    \begin{itemize}
        \item Cross-correlation computation
        \item Hysteresis threshold crossing
        \item Co-crossing event detection
        \item Synchrony metric calculation
    \end{itemize}
\end{enumerate}

\subsection{Key Code Snippets}

\subsubsection{Pose Estimation}
\begin{lstlisting}[language=Python]
from ultralytics import YOLO

# Load YOLO11n-pose model
model = YOLO('yolo11n-pose.pt')

# Process video
results = model.predict(
    source=video_path,
    stream=True,
    save=False,
    conf=0.5
)
\end{lstlisting}

\subsubsection{Identity Tracking}
\begin{lstlisting}[language=Python]
def track_identities(current_boxes, prev_boxes):
    """Assign consistent IDs using IoU matching"""
    ious = compute_iou_matrix(current_boxes, prev_boxes)
    assignments = greedy_assignment(ious)
    return assignments
\end{lstlisting}

\subsubsection{Velocity Calculation}
\begin{lstlisting}[language=Python]
def compute_velocity(keypoints):
    """Calculate frame-to-frame velocity magnitude"""
    dx = np.diff(keypoints[:, 0])
    dy = np.diff(keypoints[:, 1])
    velocity = np.sqrt(dx**2 + dy**2)
    return velocity
\end{lstlisting}

\subsubsection{Cross-Correlation}
\begin{lstlisting}[language=Python]
from scipy.signal import correlate

def compute_synchrony(signal1, signal2):
    """Compute cross-correlation between two signals"""
    correlation = correlate(signal1, signal2, mode='full')
    correlation = correlation / (len(signal1) * signal1.std() * signal2.std())
    return correlation
\end{lstlisting}

\subsection{Dependencies}
Required Python packages:
\begin{lstlisting}
torch>=2.0.0
numpy==1.26.4
opencv-python==4.10.0
ultralytics==8.3.241
matplotlib>=3.5.0
scipy>=1.9.0
\end{lstlisting}

\subsection{Installation Instructions}
\begin{lstlisting}[language=bash]
# Clone repository
git clone https://github.com/giladsar/Social_Synchrony.git
cd Social_Synchrony

# Install dependencies
pip install torch numpy opencv-python ultralytics matplotlib scipy

# Open notebook
jupyter notebook Social_Synchrony.ipynb
\end{lstlisting}

\subsection{Sample Outputs}
The \texttt{data\_output/} directory contains sample outputs:
\begin{itemize}
    \item \texttt{original\_frame\_450.png}: Raw video frame
    \item \texttt{sharpened\_frame\_450.png}: Preprocessed frame
    \item \texttt{annotated\_frame\_450.png}: Frame with pose keypoints
    \item \texttt{filter-effect\_frame\_450.png}: Visual effect demonstration
\end{itemize}

\subsection{Reproducibility}
To reproduce the analysis:
\begin{enumerate}
    \item Ensure all dependencies are installed (see above)
    \item Place video file in \texttt{./images/Test\_pose\_synchrony.mp4}
    \item Run all cells in \texttt{Social\_Synchrony.ipynb} sequentially
    \item Outputs will be saved to \texttt{data\_output/} directory
\end{enumerate}

\subsection{Contact and Support}
For questions, issues, or collaboration:
\begin{itemize}
    \item \textbf{GitHub Issues}: \url{https://github.com/giladsar/Social_Synchrony/issues}
    \item \textbf{Repository Owner}: Gilad Sar (@giladsar)
\end{itemize}

\end{document}
